<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>~agentydragon</title>
        <link>http://agentydragon.com</link>
        <description><![CDATA[*agentydragon noises*]]></description>
        <atom:link href="http://agentydragon.com/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Tue, 20 Jul 2021 00:00:00 UT</lastBuildDate>
        <item>
    <title>Setting up Bazel on GitLab CI</title>
    <link>http://agentydragon.com/posts/2021-07-20-bazel-gitlab.html</link>
    <description><![CDATA[<p>Iâ€™ve started migrating my projects to <a href="https://gitlab.com/agentydragon">GitLab</a>, which I like more than GitHub, because the core is actually open-source and you can run your own instance. I use <a href="https://bazel.build">Bazel</a> in most of my projects, and set it up to test some of my projects on pull requests. Hereâ€™s how I set it up.</p>
<h2 id="overview">Overview</h2>
<p>Some of my projects have fairly large dependencies, and I donâ€™t want to spend the time installing them on the container on every CI run. So I have my own CI Dockerfile.</p>
<p>On every commit, I try to build a new CI image, as part of CI. The CI image is stored in <a href="">the projectâ€™s Container Registry</a> ##TODO##. Only parts of the Dockerfile that need rebuilding will be rebuilt - I tell Docker to use the latest built image as a cache.</p>
<h2 id="dockerfile">Dockerfile</h2>
<p>predefined variables: https://docs.gitlab.com/ee/ci/variables/predefined_variables.html</p>
<pre><code>FROM ubuntu:latest

# Adding the Bazel signing key needs curl and gnupg2.
RUN DEBIAN_FRONTEND=noninteractive apt-get update &amp;&amp; \
  apt-get full-upgrade -y &amp;&amp; \
  apt-get install -y \
    curl \
    gnupg2

RUN curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
RUN echo &quot;deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8&quot; &gt; /etc/apt/sources.list.d/bazel.list

RUN apt-get update
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y \
  bazel \
  python-is-python3 \
  python3-pip

# Cleanup.
RUN apt-get clean &amp;&amp; \
  apt-get autoremove -y</code></pre>]]></description>
    <pubDate>Tue, 20 Jul 2021 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2021-07-20-bazel-gitlab.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>Rai's ML mistakes, part 2 of âˆ</title>
    <link>http://agentydragon.com/posts/2021-01-04-rai-ml-mistakes-2.html</link>
    <description><![CDATA[<p>Continuing my list of ML mistakes from <a href="/posts/2020-12-31-cartpole-q-learning.html">last time</a>, hereâ€™s:</p>
<h1 id="rais-ml-mistake-4-too-much-autograd">Raiâ€™s ML mistake #4: Too much autograd</h1>
<p>So here I am, writing an agent for <a href="https://gym.openai.com/envs/LunarLander-v2/">LunarLander-v2</a>. Iâ€™m using Q-learning. I approximate <span class="math inline"><em>q</em><sub>*</sub>(<em>s</em>,â€†<em>a</em>)</span> as <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>s</em>,â€†<em>a</em>)</span> with a neural network, taking a vector representing the state, and outputting one approximate action value per output. The neural network is trained to minimize squared TD error on the policy the agentâ€™s running, which is <span class="math inline"><em>Îµ</em></span>-greedy with respect to <span class="math inline"><em>qÌ‚</em></span>: <br /><span class="math display">$$
\begin{align*}
\require{extpfeil}
\ell(w) &amp;= \mathop{\mathbb{E}}\limits_{(S \xrightarrow{A} R,S') \sim \mathrm{greedy}(\mathrm{\hat{q}}_w)}
\left[ \left(\mathrm{\hat{q}}_w(S, A) - (R + \gamma \max_{A'} \mathrm{\hat{q}}_w(S',A')) \right)^2 \right] \\
\text{output } &amp;\arg\min_w \ell(w)
\end{align*}
$$</span><br /></p>
<h2 id="not-quite-off-policy">Not quite off-policy</h2>
<p>One note about the â€œpolicityâ€ of this method.</p>
<p>Tabular Q-learning without function approximation is off-policy - you learn about <span class="math inline"><em>Ï€</em><sub>*</sub></span> from experience <span class="math inline">(<em>S</em>â†’<sub><em>A</em></sub><em>R</em>,â€†<em>S</em>â€²)</span> sampled from any (saneâ„¢) policy. You just keep updating <span class="math inline"><em>qÌ‚</em>(<em>S</em>,â€†<em>A</em>)</span> towards <span class="math inline"><em>R</em>â€…+â€…<em>Î³</em>â€…â‹…â€…max<sub><em>A</em>â€²</sub><em>qÌ‚</em>(<em>S</em>â€²,â€†<em>A</em>â€²)</span>, and to <span class="math inline">maxâ€†</span> is there because you want to learn about the optimal policy.</p>
<p>But note that in <span class="math inline">â„“(<em>w</em>)</span>, the experience <span class="math inline">(<em>S</em>â†’<sub><em>A</em></sub><em>R</em>,â€†<em>S</em>â€²)</span> is sampled from the policy <span class="math inline"><em>g</em><em>r</em><em>e</em><em>e</em><em>d</em><em>y</em>(<em>qÌ‚</em><sub><em>w</em></sub>)</span>. We need to expect <em>over a policy</em>, because weâ€™re using function approximation, so presumably we cannot learn a <span class="math inline"><em>w</em></span> which would make <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub></span> exactly fit <span class="math inline"><em>q</em><sub>*</sub></span>. So we have to pick out battles for how well do we approximate <span class="math inline"><em>q</em><sub>*</sub></span> - we care about approximating it closely for states and actions actually visited by the estimation policy.</p>
<p>Instead of assuming that we can sample <span class="math inline">(<em>S</em>â†’<sub><em>A</em></sub><em>R</em>,â€†<em>S</em>â€²)</span> from <span class="math inline"><em>g</em><em>r</em><em>e</em><em>e</em><em>d</em><em>y</em>(<em>qÌ‚</em><sub><em>w</em></sub>)</span> (so that we can approximate the expected squared TD error over it), I guess you could use the general importance sampling recipe to get rid of that: <br /><span class="math display">$$\mathop{\mathbb{E}}_\limits{X\sim \pi}[\mathrm{f}(X)] =
\mathop{\mathbb{E}}_\limits{X\sim b}\left[\mathrm{f}(X) \cdot \frac{\pi(X)}{b(X)}\right]$$</span><br /></p>
<h2 id="semi-gradient">Semi-gradient</h2>
<p>So, we want to minimize <span class="math inline">â„“</span>.</p>
<p>Note that <span class="math inline">â„“</span> depends on <span class="math inline"><em>w</em></span> (via <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub></span>) in 3 places:</p>
<ol type="1">
<li>In <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>,â€†<em>A</em>)</span>, which we are trying to nudge to move to the right place,</li>
<li>in <span class="math inline"><em>R</em>â€…+â€…<em>Î³</em>max<sub><em>A</em>â€²</sub><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>â€²,â€†<em>A</em>â€²)</span>, which is a sample from a distribution centered on <span class="math inline"><em>q</em><sub><em>g</em><em>r</em><em>e</em><em>e</em><em>d</em><em>y</em>(<em>qÌ‚</em><sub><em>w</em></sub>)</sub>(<em>S</em>,â€†<em>A</em>)</span>,</li>
<li>and in the distribution weâ€™re taking the expectation on.</li>
</ol>
<p>In practice, we hold (2) and (3) constant, and in one optimization step, we wiggle <span class="math inline"><em>w</em></span> only to move <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>,â€†<em>A</em>)</span> closer to targets. That means that in our gradient, we are ignoring the dependency of (2) and (3) on the <span class="math inline"><em>w</em></span> that we are optimizing, which makes this not a full gradient method, but instead a <em>semi-gradient</em> method.</p>
<h2 id="experience-replay">Experience replay</h2>
<p>My first shot at this agent just learned from 1 step (sampled from <span class="math inline"><em>Îµ</em></span>-greedy policy for <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub></span>) at a time. It worked in the sense that it ended up learning a policy close enough to â€œsolving the environmentâ€. (The environment says the â€œsolved rewardâ€ is 200. I got maybe like 150-180 over 100 episodes, so not quite there, but itâ€™s close enough for me to say â€œmeh, Iâ€™ll wiggle a few hyperparameters and get thereâ€.)</p>
<p>But to learn a fair policy, it took the agent about 10 000 episodes, and the per-episode total reward over time made a spiky ugly graph:</p>
<figure>
<img src="/static/2020-12-31-total_reward.svg" style="height: 400px;"
     title="Total reward per episode graph">
</figure>
<p>I donâ€™t like that it takes all of 10 000 episodes, and I donâ€™t like how spiky and ugly the chart is.</p>
<p>Experience replay means we store a bunch of experience <span class="math inline">(<em>S</em>â†’<sub><em>A</em></sub><em>R</em>,â€†<em>S</em>â€²)<sub>1,â€†2,â€†â€¦</sub></span> in a buffer, and instead of updating <span class="math inline"><em>w</em></span> by some gradient-based optimization method (I used ADAM) to minimize squared TD error one step at a time, we update it to minimize squared TD error over the whole buffer, a bunch of steps at a time.</p>
<p>Experience replay should make learning more sample-efficient (so it should need less than 10 000 episodes). Also, it should reduce one source of â€œspikiness and uglinessâ€ in the chart, because the chart will be doing step updates on a larger batch. Making the batch larger should reduce the variance of the updates.</p>
<h2 id="broken-code">Broken code</h2>
<p>So, hereâ€™s how I initially implemented one step of the update. <code>self.experience_{prestates, actions, rewards, poststates, done}</code> holds the experience buffer (<span class="math inline"><em>S</em>,â€†<em>A</em>,â€†<em>R</em>,â€†<em>S</em>â€²</span> respectively for observed transition <span class="math inline"><em>S</em>â†’<sub><em>A</em></sub><em>R</em>,â€†<em>S</em>â€²</span>, plus flag to signal end of episode).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="at">@tf.function</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">def</span> q_update(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">  <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">    <span class="co"># \max_{A&#39;} \hat{q}(S&#39;, A&#39;)</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">    best_next_action_value <span class="op">=</span> tf.reduce_max(</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">        <span class="va">self</span>.q_net(<span class="va">self</span>.experience_poststates), axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    <span class="co"># If episode ends after this step, the environment will only give us</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">    <span class="co"># one step of reward and nothing more. Otherwise, the value of the next</span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    <span class="co"># state S&#39; is best_next_action_value.</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10">    next_state_value <span class="op">=</span> tf.where(</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">        <span class="va">self</span>.experience_done,</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">        tf.zeros_like(best_next_action_value),</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">        best_next_action_value)</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">    targets <span class="op">=</span> <span class="va">self</span>.experience_rewards <span class="op">+</span> <span class="va">self</span>.discount_rate <span class="op">*</span> next_state_value</a>
<a class="sourceLine" id="cb1-15" data-line-number="15"></a>
<a class="sourceLine" id="cb1-16" data-line-number="16">    <span class="co"># For all states S_i in the experience buffer, compute Q(S_i, *) for all</span></a>
<a class="sourceLine" id="cb1-17" data-line-number="17">    <span class="co"># actions.</span></a>
<a class="sourceLine" id="cb1-18" data-line-number="18">    next_action_values <span class="op">=</span> <span class="va">self</span>.q_net(<span class="va">self</span>.experience_prestates)</a>
<a class="sourceLine" id="cb1-19" data-line-number="19">    <span class="co"># Select Q(S_i, A_i) where A_i corresponds to the recorded experience</span></a>
<a class="sourceLine" id="cb1-20" data-line-number="20">    <span class="co"># S_i --(A_i)--&gt; R_i, S&#39;_i, done_i.</span></a>
<a class="sourceLine" id="cb1-21" data-line-number="21">    indices <span class="op">=</span> tf.stack(</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">      (tf.<span class="bu">range</span>(<span class="va">self</span>.experience_buffer_size), <span class="va">self</span>.experience_actions),</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">      axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">    values_of_selected_actions <span class="op">=</span> tf.gather_nd(next_action_values, indices)</a>
<a class="sourceLine" id="cb1-25" data-line-number="25"></a>
<a class="sourceLine" id="cb1-26" data-line-number="26">    loss <span class="op">=</span> tf.keras.losses.MeanSquaredError()(</a>
<a class="sourceLine" id="cb1-27" data-line-number="27">        values_of_selected_actions, targets)</a>
<a class="sourceLine" id="cb1-28" data-line-number="28"></a>
<a class="sourceLine" id="cb1-29" data-line-number="29">  grad <span class="op">=</span> tape.gradient(loss, <span class="va">self</span>.q_net.trainable_variables)</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">  <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grad, <span class="va">self</span>.q_net.trainable_variables))</a></code></pre></div>
<p>Whatâ€™s wrong here?</p>
<p>The symptom is that the policy is not improving. The total reward per episode is just oscillating.</p>
<figure>
<img src="/static/2020-sticker-hmm.png" title="Hmm">
</figure>
<h2 id="the-problem">The problem</h2>
<p>Remember how I said itâ€™s a <em>semi-gradient</em> method?</p>
<p>Hereâ€™s the fix:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="at">@tf.function</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">def</span> q_update(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">  <span class="co"># \max_{A&#39;} \hat{q}(S&#39;, A&#39;)</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">  best_next_action_value <span class="op">=</span> tf.reduce_max(</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">      <span class="va">self</span>.q_net(<span class="va">self</span>.experience_poststates), axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">  <span class="co"># If episode ends after this step, the environment will only give us</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">  <span class="co"># one step of reward and nothing more. Otherwise, the value of the next</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">  <span class="co"># state S&#39; is best_next_action_value.</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9">  next_state_value <span class="op">=</span> tf.where(</a>
<a class="sourceLine" id="cb2-10" data-line-number="10">      <span class="va">self</span>.experience_done,</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">      tf.zeros_like(best_next_action_value),</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">      best_next_action_value)</a>
<a class="sourceLine" id="cb2-13" data-line-number="13">  targets <span class="op">=</span> <span class="va">self</span>.experience_rewards <span class="op">+</span> <span class="va">self</span>.discount_rate <span class="op">*</span> next_state_value</a>
<a class="sourceLine" id="cb2-14" data-line-number="14"></a>
<a class="sourceLine" id="cb2-15" data-line-number="15">  <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</a>
<a class="sourceLine" id="cb2-16" data-line-number="16">    <span class="co"># For all states S_i in the experience buffer, compute Q(S_i, *) for all</span></a>
<a class="sourceLine" id="cb2-17" data-line-number="17">    <span class="co"># actions.</span></a>
<a class="sourceLine" id="cb2-18" data-line-number="18">    next_action_values <span class="op">=</span> <span class="va">self</span>.q_net(<span class="va">self</span>.experience_prestates)</a>
<a class="sourceLine" id="cb2-19" data-line-number="19">    <span class="co"># Select Q(S_i, A_i) where A_i corresponds to the recorded experience</span></a>
<a class="sourceLine" id="cb2-20" data-line-number="20">    <span class="co"># S_i --(A_i)--&gt; R_i, S&#39;_i, done_i.</span></a>
<a class="sourceLine" id="cb2-21" data-line-number="21">    indices <span class="op">=</span> tf.stack(</a>
<a class="sourceLine" id="cb2-22" data-line-number="22">      (tf.<span class="bu">range</span>(<span class="va">self</span>.experience_buffer_size), <span class="va">self</span>.experience_actions),</a>
<a class="sourceLine" id="cb2-23" data-line-number="23">      axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2-24" data-line-number="24">    values_of_selected_actions <span class="op">=</span> tf.gather_nd(next_action_values, indices)</a>
<a class="sourceLine" id="cb2-25" data-line-number="25"></a>
<a class="sourceLine" id="cb2-26" data-line-number="26">    loss <span class="op">=</span> tf.keras.losses.MeanSquaredError()(</a>
<a class="sourceLine" id="cb2-27" data-line-number="27">        values_of_selected_actions, targets)</a>
<a class="sourceLine" id="cb2-28" data-line-number="28"></a>
<a class="sourceLine" id="cb2-29" data-line-number="29">  grad <span class="op">=</span> tape.gradient(loss, <span class="va">self</span>.q_net.trainable_variables)</a>
<a class="sourceLine" id="cb2-30" data-line-number="30">  <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grad, <span class="va">self</span>.q_net.trainable_variables))</a></code></pre></div>
<p>So, what was the problem?</p>
<p>The code calls the Q network twice: once to compute the targets (<span class="math inline"><em>R</em>â€…+â€…<em>Î³</em>â€…â‹…â€…max<sub><em>A</em>â€²</sub><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>â€²,â€†<em>A</em>â€²)</span>), once to compute <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>,â€†<em>A</em>)</span>. Then, we will compute a loss, and we will take its partial <em>â€œsemi-derivativeâ€</em> with respect to <span class="math inline"><em>w</em></span>, and apply the gradient to bring <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>,â€†<em>A</em>)</span> closer to the target.</p>
<p>The problem was: I also put the target computation into <code>GradientTape</code> scope, so the optimization was given the freedom to change not just <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>,â€†<em>A</em>)</span>, but <em>also</em> <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub>(<em>S</em>â€²,â€†<em>A</em>â€²)</span>. So the fix was just to move computing the targets out of the <code>GradientTape</code> scope.</p>
<p>I looked at this code basically non-stop for 2 hours, and I realized the error when I took a break and talked with a friend.</p>
<figure>
<img src="/static/2020-sticker-ded.png" title="_(x.x)_   <-- ded">
</figure>
<h2 id="pet-peeve-47-math-typesetting">Pet peeve #47: math typesetting</h2>
<p><em>The full list of previous 46 pet peeves will be provided on request, subject to a reasonable processing fee.</em></p>
<h3 id="mathjax-hat-and-mathrm">MathJax, <code>\hat</code> and <code>\mathrm</code></h3>
<p><span class="math inline"><em>qÌ‚</em></span> is a function (of <span class="math inline"><em>w</em>,â€†<em>S</em>,â€†<em>A</em></span>), not a variable, so it shouldnâ€™t be typeset in italic. I tried using <code>\hat{\mathrm{q}}_w</code>. I believe that works in LaTeX but turns out that MathJax is not willing to render it (<span class="math inline">$\hat{\mathrm{q}}$</span>). But <code>\mathrm{\hat{q}}</code> is perfectly fine: <span class="math inline"><em>qÌ‚</em></span>. But <code>\mathrm{\hat{q}}_w</code> is perfectly fine: <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub></span>.</p>
<h3 id="mathjax-and-inline-xrightarrow">MathJax and inline <code>\xrightarrow</code></h3>
<p>Also, my MathJax doesnâ€™t seem to understand <code>\xrightarrow</code> in inline equations. Thatâ€™s a shame, because <code>S \xrightarrow{A} R, S'</code> is more readable than <code>S \rightarrow_A R, S'</code> (<span class="math inline"><em>S</em>â†’<sub><em>A</em></sub><em>R</em>,â€†<em>S</em>â€²</span>), which I used here instead (in inline equations). It looks like this: <br /><span class="math display">$$S \xrightarrow{A} R, S'$$</span><br /> Let me know if you know whatâ€™s up with those MathJax things. I wonder if itâ€™s MathJax being wrong, or me sucking at LaTeX.</p>
<h3 id="why-im-a-math-typesetting-snob">Why Iâ€™m a math typesetting snob</h3>
<p>Typesetting things that arenâ€™t variables as if they were variables really bugs me, because it makes the formulas really ugly. And the font you use to typeset a math thing is a very useful hint for the reader about what sort of object it is. I learned a bit about it when volunteering as a <a href="https://ksp.mff.cuni.cz/">KSP</a> organizer - KSP is full of math snobs. Compare: <br /><span class="math display">$$
\begin{align*}
\mathrm{Loss}(w) = \sum_i (\mathrm{f}(x_i) - y_i)^2 \\
Loss(w) = \sum_i (f(x_i) - y_i)^2
\end{align*}$$</span><br /> In the second one, it takes a bit of processing to understand that <span class="math inline"><em>L</em><em>o</em><em>s</em><em>s</em></span> is not a multiplication (<span class="math inline"><em>L</em>â€…â‹…â€…<em>o</em>â€…â‹…â€…<em>s</em>â€…â‹…â€…<em>s</em></span>), and that <span class="math inline"><em>f</em>(<em>x</em><sub><em>i</em></sub>)</span> is function application.</p>
<p>If you want to read more, you can take a look at <a href="https://en.wikipedia.org/wiki/Typographical_conventions_in_mathematical_formulae">Typographical conventions in mathematical formulae on Wikipedia</a>. Or maybe some LaTeX / TeX books or reference material might have a lot of explanations, like â€œuse this in these situationsâ€. And also good math books often have a large table at the front which explains used conventions, like â€œ<span class="math inline"><strong>w</strong></span> is a vector, <span class="math inline"><em>X</em></span> is a matrix, <span class="math inline"><em>f</em></span> is a function, â€¦â€</p>
<figure>
<img src="https://imgs.xkcd.com/comics/kerning.png" style="height: 400px;"
     title="XKCD 1015 (Kerning)">
<div>
<p><a href="https://xkcd.com/1015/">https://xkcd.com/1015/</a> <br> Now you know about ugly errors in math typesetting, and if you Google it, also about bad kerning. Youâ€™re welcome, pass it along.</p>
</div>
</figure>
<figure>
<img src="/static/2020-sticker-mlem.png" title="Mlem!">
</figure>]]></description>
    <pubDate>Mon, 04 Jan 2021 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2021-01-04-rai-ml-mistakes-2.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>Cartpole Q-learning</title>
    <link>http://agentydragon.com/posts/2020-12-31-cartpole-q-learning.html</link>
    <description><![CDATA[<p>Recently Iâ€™ve been working on skilling up on reinforcement learning, particularly practice. Iâ€™m currently on the last course of the <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning specialization</a> from University of Alberta on Coursera. The last piece of the course is about solving the <a href="https://gym.openai.com/envs/LunarLander-v2/">Lunar Lander</a> environment. Iâ€™ve been trying to solve it on my own first before going through the labs, so that I can learn things deeper and experiment.</p>
<p>Iâ€™ve tried implementing an actor-critic agent. The actor is a feed-forward neural network specifying a parameterized policy <span class="math inline"><em>Ï€</em><sub><em>Î¸</em></sub></span>. The networkâ€™s input is a representation of the state, and it has one output per action. The policy is a softmax over these outputs. I tried a critic for predicting both <span class="math inline"><em>vÌ‚</em><sub><em>w</em></sub></span>, and <span class="math inline"><em>qÌ‚</em><sub><em>w</em></sub></span>.</p>
<p>Iâ€™ve not had good luck getting this to work so far. At one point I got the agent to fly above the surface (without landing), but then later I edited the code somewhat, aaaaand it was gone.</p>
<p>I stared a bunch into my update equations, but have not been able to find any obvious errors. I used TensorFlowâ€™s <a href="https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams">HParams</a> to try to tune all the hyperparameters, like actor learning rate, critic learning rate, and learning rate for the average reward. (I wrote it attempting to use the continuing average reward formulation.)</p>
<p>I decided to first try a simpler environment, <a href="https://gym.openai.com/envs/CartPole-v0/">CartPole</a>.</p>
<p>In the end, I managed to solve it a couple hours back.</p>
<p>In the implementation, Iâ€™ve made a couple mistakes and observations, which I want to note down.</p>
<h2 id="colab-notebook-with-my-cartpole-agent">Colab notebook with my CartPole agent</h2>
<p>Hereâ€™s my notebook if you want to play around:</p>
<p><a href="https://colab.research.google.com/github/agentydragon/agentydragon.github.io/blob/devel/notebooks/2020-12-31-cartpole-q-learning.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<h2 id="rais-ml-mistake-1-short-episodic-environments-can-use-high-Î³">Raiâ€™s ML mistake #1: Short episodic environments can use high Î³</h2>
<p>I initially wrote my code to use a discount rate of 0.9. On <a href="https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/">n1tryâ€™s solution</a> that I found on <a href="https://gym.openai.com/envs/CartPole-v0/">the leaderboard</a> (unfortunately not sure which one it was), the discount rate was actually set to 1.</p>
<p>I suspect I might have set the discount rate too low. The CartPole environment has episodes which have length of only up to ~500, with 1 unit of reward per step.</p>
<p>If you have a discount rate Î³ and an average per-step reward of <span class="math inline"><em>r</em></span>, then in an infinite environment, the value of a state will be something like: <br /><span class="math display">$$ \frac{r}{1-\gamma} = r + r \cdot \gamma + r \cdot \gamma^2 $$</span><br /> Knowing this, I was a worried that if I set <span class="math inline"><em>Î³</em></span> too high, the targets for the Q network to learn would have a high variance. But I forgot that the environment had only like ~500 steps, so setting <span class="math inline"><em>Î³</em>â€„=â€„1</span> would be alright in this case.</p>
<p><em>Lesson learned</em>: I need to keep in mind the environmentâ€™s characteristics, in particular how long are the episodes and how high total rewards can I expect.</p>
<h2 id="rais-ml-mistake-2-too-little-exploration">Raiâ€™s ML mistake #2: Too little exploration</h2>
<p>The algorithm that ended up working for me was Q-learning (with function approximation by a small neural network). I selected actions <span class="math inline"><em>Îµ</em></span>-greedily, with <span class="math inline"><em>Îµ</em></span> set to 0.02, so ~2% chance of random moves.</p>
<p>Looking at some solutions of the environment that I found, they had much higher exploration rates. Some that I saw had 100% random actions initially, and had it then decay. And the particular solution I was looking at set the minimal exploration rate, after all the decay, to <em>10%</em> - 5x more than I had.</p>
<p>I think my code found a policy that â€œsolvesâ€ the environment faster when I put in 10% exploration.</p>
<h2 id="rais-ml-mistake-3-evaluation-interfering-with-training">Raiâ€™s ML mistake #3: Evaluation interfering with training</h2>
<p>I ran my algorithm on 10k episodes, and every 1k episodes, I ran the code in â€œgreedy modeâ€ (i.e., no random actions) and recorded average performance on 100 episodes. I did that because my Q-learning implementation was executing an <span class="math inline"><em>Îµ</em></span>-soft policy, which might be worse than the greedy policy that itâ€™s learning. I donâ€™t know how â€œfragileâ€ the CartPole environment is (i.e., how much worse the total reward per episode gets if I force an agent to take some amount of random actions), but I wanted to rule it out as a source of errors.</p>
<p>I implemented the evaluation by just adding a flag <code>train: bool = True</code> to the agentâ€™s functions. If <code>train</code> was <code>False</code>, Iâ€™d skip all the update steps and select actions greedily.</p>
<p>Unfortunately, I ended up making a mistake and I forgot to add the condition around one branch of code - updating the Q network after a final step (i.e., when the agent receives a last reward and the episode ends).</p>
<p>As a result, the agent ended up executing ~100 incorrect updates (based on an incorrect last action and state, towards the final reward), one incorrect update per evaluation episode.</p>
<p><strong>Lesson learned</strong>: Double check my code? Maybe even test my code? Not sure how to learn from this :/</p>
<h2 id="forgetting-when-learned-too-much"><em>Forgetting when learned too muchâ€½â€½</em></h2>
<figure>
<img src="/static/2020-12-31-total_reward.svg" style="height: 400px;"
     title="Total reward per episode graph">
<div>
<p><a href="https://www.youtube.com/watch?v=sIlNIVXpIns"><em>ğ„ â™« Look at this graaaphâ™« ğ„»</em></a></p>
</div>
</figure>
<p>So, until episode maybe 400 or so, nothing much is happening. Then until about step 1800, itâ€™s sorta doing something but could be better. Then at around step 1800, it finds a good policy, and returns are nice. Basically, problem solved.</p>
<p>Then I train it for a bit longer, and at episode ~2.2kâ€¦ for some reason performance goes WAY down for about 300 or so episodes. Itâ€™s as bad as if the network forgot everything it learned before.</p>
<p>Then, after a while (at about 2700), it quickly climbs back up to good performance. On the full graph with 10k episodes, this cycle would repeat maybe every 2000-3000 episodes. Wat.</p>
<p>I have no idea whatâ€™s going on here. Maybe one sort-of ideas, but I have not tested it, and I have a relatively low degree of confidence in it.</p>
<p>Maybe for some reason the critic might overfit in some way that makes it behave badly on some early action. Maybe itâ€™s trying to learn some relatively â€œlate gameâ€ stuff, and the update that happens ends up screwing some early behavior, so the agent then has to spend a bunch of episodes learning the right early behavior again. The policy changes in a non-continuous way, so if some early decision switches to do the wrong thing, the agent will then have to follow a bunch of wrong trajectories until the one-step Q updates end up bubbling up to the initial wrong decision. I guess this might be somewhat mitigated by using eligibility traces so that updates bubble up faster, or by using actor-critic with soft policies.</p>
<p>Another potential reason for having a really bad episode might be if the agent happens to pick a random action (with probability <span class="math inline"><em>Îµ</em></span>) at an early point where the pole is unstable and very easy to mess up. And then it canâ€™t recover from that error. But that explanation isnâ€™t supported by how wide these areas of low episode returns are. It might explain maybe one sporadic bad episode, but not a whole bunch of them after each other.</p>
<h2 id="next-steps">Next steps</h2>
<p>Now that I got a CartPole agent running, Iâ€™ll come back to the Lunar Lander environment. Iâ€™ll first try solving it again with a Q network. I could probably similarly get away with not discounting rewards at all (<span class="math inline"><em>Î³</em>â€„=â€„1</span>).</p>
<p>Also Iâ€™d like to implement experience replay to make this more sample-efficient.</p>
<p>If that ends up working, I still want to get actor-critic working.</p>
<p>Obligatory scoot-scoot:</p>
<video controls loop autoplay>
<source src="/static/2020-12-31-cartpole.mp4" type="video/mp4">
</video>]]></description>
    <pubDate>Thu, 31 Dec 2020 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2020-12-31-cartpole-q-learning.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>Playing with AI</title>
    <link>http://agentydragon.com/posts/2020-11-23-playing-with-ai.html</link>
    <description><![CDATA[<style>
.highlight-on .generated {
  background-color: yellow;
}
</style>
<div id="playing-with-ai-toggle-highlight">
<p>The last about 2 weeks I have taken some time to finally get practical AI experience, so Iâ€™m running TensorFlow and all, and making lots of Anki cards.</p>
<h2 id="sidenote-anki-for-programming-is-awesome">Sidenote: Anki for programming is awesome!</h2>
<p>By the way, Anki cards are so useful for learning how to use libraries fluently without looking things up itâ€™s ridiculous.</p>
<p>For example, thanks to a bunch of Clozes, if you give me a CSV dataset, I can define, train and evaluate a network using the Keras functional API, without looking up anything. It means I get quickly to the interesting stuff, and donâ€™t waste 80% of my time looking up things I already looked up before. If I see that Iâ€™m looking something up for, say, the 3rd time, I just make the cards that would have helped me, and that might be the last time I look that up.</p>
<h2 id="can-a-computer-now-write-similarly-well-to-me">Can a computer now write similarly well to me?</h2>
<p>I saw very interesting examples of how well modern language model can generate text, and Iâ€™m wondering how close can I be emulated at this point. Depending on how good a language model is, it could replace the copywriting industry and make a 1000% profit margin on top. And I sort of wonder how close it is to me. Though Iâ€™m not sure what would I do if I learned my writing can be replaced. I guess I would continue enjoying it, because it feels like Iâ€™m â€œexpressing my soulâ€, and also itâ€™s useful to sharpen my thoughts. If my understanding of something is foggy, when I write things down, I can much more easily spot where exactly Iâ€™m confused, or whatâ€™s a question I canâ€™t answer, or to realize â€œhey actually I made a logical error, I no longer believe the conclusion I wanted to justifyâ€.</p>
<p>But I guess I could then just as well automate this whole website thing. I put too little stuff on it anyway - if I just got myself to do the work and put my ideas into polished-ish writing, I would write so much more.</p>
<p>I guess Iâ€™d still write also because thereâ€™s some pleasure in someone telling me â€œoh by the way I read your article the other day, thanks for that tipâ€. And Iâ€™d not get that from text I had a Transformer write. Even if it did manage to write a thing as good as me or better, so that people would compliment me for â€œhey nice thing on your websiteâ€, it would still make me go a bit â€œnice!â€, but it would ring a little hollow, I guess. Praise for work that isnâ€™t mine. But then, did the Transformer really â€œworkâ€ for it? Also the work coming up with the architecture and implementing it and making <a href="https://transformer.huggingface.co/doc/distil-gpt2">Write With Transformer</a> belongs to many many other people.</p>
<h2 id="experiment">Experiment</h2>
<p>So Iâ€™m going to try it in this article. I will start slowly replacing words by top suggestions (<a href="https://transformer.huggingface.co/doc/distil-gpt2">Write With Transformer distil-gpt2</a>). Iâ€™ll start with maybe 90% me, 10% Transformer, and by the time I finish writing this, itâ€™ll be 100% Transformer. And I wonâ€™t, at least for now, tell you which parts are me and which are the generator. That way I wonâ€™t have just a test of â€œI can / cannot be replaced by a Transformerâ€, but by asking people which sentences were from me and which were from the Transformer, Iâ€™ll get more gradual information about the point at which todayâ€™s Transformers can replace me. From what I read, models like GPT-3 are able to convincingly copy â€œsurface styleâ€, and they are able to make simple inferences, but they might make mistakes.</p>
<p>By the way, the footer of <a href="https://transformer.huggingface.co/">Write With Transformer</a> says: â€œIt is to writing what calculators are to calculus.â€ And thatâ€™s a nice sentence in that, on a shallow reading, it sounds like a positive comparison. â€œIt is to writing what [things for doing mathematics] are to [field of mathematics].â€ But I never had a calculator that was any good for calculus. I never saw a calculator with a â€œderive by xâ€ or â€œindefinite integral dxâ€. Though now I also wonder why no calculator has it. It would be so useful, and not that hard to implement. Mathematica can integrate most of what you throw at it! And algorithms for integrating broad classes of functions are also totally a thing in literature!</p>
<p>â€œIt is to writing what Mathematica is to calculusâ€? Sure. That sounds useful. A tool that can solve 90% of practical problems. Neat. â€œIt is to writing what calculators are to calculusâ€? AAA, <span class="generated">you know what? Iâ€™ve been</span> in many situations where you can have all the calculators you want, and they wonâ€™t save you from an ugly enough integral.</p>
<p>It sounds like one of those â€œproverb insultsâ€, like â€œyou must be at the top of the Gauss curveâ€.</p>
<p>Also the test of making the Transformer generate parts of the article can show if it could be useful as a computer-assisted authoring tool.</p>
<p><span class="generated">I wonder what a problem is</span> there with making the jobs of some people much easier with AI like this. For example, I have a virtual assistant, and I think it should be possible to augment them with a Transformer. You train the Transformer on chats of the highest rated virtual assistants with customers, and annotate the conversations with times when the virtual assistant had to do something. Then you integrate that Transformer into, say, Google Chat, and add some quick shortcuts, like â€œTabâ€ for â€œautocompleteâ€. I fully expect we should be able to mostly automate conversation like â€œhello, I hope youâ€™re having a nice dayâ€.</p>
<h2 id="motivation-to-do-my-own-thing">Motivation to do my own thing</h2>
<p>By the way, the other day I stumbled on <a href="https://betterexplained.com/">Better Explained</a>, and the author has a great article: Surviving (and thriving) on your own: Know Thyself.</p>
<p>And this <span class="generated">is the best of</span> sources of motivation to do my own thing that Iâ€™ve seen in a while. This article made me realize that yes, there are actually things I want to do. I can just look at my TODO list in my Roam Research database. If I only had the most productive ~8 hours of my time available for all the things I want to learn and make.</p>
<p>So Iâ€™ve been considering going part-time at Google. <span class="generated">For some time Iâ€™ve found that I</span> just canâ€™t muster the energy to be consistently productive after work. And switching contexts is expensive, and gets much more expensive when you switch to a context you havenâ€™t seen for a couple days. Like what might happen if one day I do 4 hours of Colab experimentation, then the next week I donâ€™t have spare energy after work, and then a week later I open the notebook again, read and go ğŸ¤¨. It helps to keep notes, and not half-ass the code style too badly, but thereâ€™s a trade-off between â€œhelp future-me quickly get back to productivity on this taskâ€ and â€œspend energy making progressâ€.</p>
<p>Also, with stocks having recovered from COVID and with Bitcoin currently going through Another One Of Those, Iâ€™ve been closely watching how much longer until financial independence.</p>
<p>I am not at the point of â€œIâ€™ll be able to live on this on my current standard foreverâ€. But at a lower standard? Like moving back to Czech Republic? For some values of â€œlower standardâ€, yes. And at some some point, the marginal expected gains from a higher monthly budget will become dominated by the gains of having more free time. And that <span class="generated">makes</span> it less and less rational to trade financial security for freedom.</p>
<p>And itâ€™s not like I canâ€™t make money doing things I want to do, either. Thereâ€™s such a huge spectrum. I can career-steer at Google more boldly, or go part-time to do my own thing. Or even <span class="generated">if itâ€™s a little less expensive</span> It might not be just the same freedom as reducing my hours, or working with a company thatâ€™s more closely aligned with my interests, or even quitting and doing my thing.</p>
<h2 id="still-experimenting">Still experimenting</h2>
<p>By the way, Iâ€™m still doing the GPT-3 <span class="generated">of this</span> article thing, with slowly increasing density. And as I increase the density, I expect <span class="generated">it will be more â€œefficientâ€ to</span> just output tons of filler text. Filler is used in more articles than crisp sentences that have only narrow meaning. If GPT-3 outputs â€œWhat gives?â€ after any of my sentences, it will probably get a higher reward than if it outputs â€œan endomorphism that is an isomorphism is an automorphismâ€, just because itâ€™s <span class="generated">all too hard to get some extra filler</span> into a place where it would not plausibly fit. What gives?</p>
<p>So expect this piece of writing to slowly degrade from saying something to saying nothing in so many words. <span class="generated">And Iâ€™m doing it in my mind</span> expecting some successive Tab press to generate either nonsense, or bullshit. I <span class="generated">m going to continue to keep it in</span> line with making sense, as far as Iâ€™m able to express myself through filler text.</p>
<p><span class="generated">As a result, I think this will</span> sometimes end up stating things I donâ€™t actually endorse or believe. If that happens, I think Iâ€™ll also release the â€œspoilerâ€ (annotating the sections of this text that are generated) immediately, so that I donâ€™t accidentally say something like â€œI for one welcome our AI overlordsâ€. Well, <span class="generated">I am sure I will</span> As long as we as a species donâ€™t fail the exam.</p>
<p><span class="generated">As far as Iâ€™m concerned, I will continue to put together articles on</span> whatever interests me, to write code for problems I want solved, and to try to improve my habits. If the worldview and â€œvalues-viewâ€ and â€œlifestyleâ€ that I want to implement sticks, <span class="generated">then the same can be said for every</span> positive change itâ€™s brought the past couple weeks. <span class="generated">So, whatâ€™s really causing this</span> to slip away slowly? Why have previous times when I held similar viewpoints slipped back into routine? Maybe itâ€™s just because itâ€™s been the default for so long for me to work <span class="generated">through the things I like</span> Slowly, <span class="generated">slowly</span> because of all the panic from ideas like â€œmaybe I might have to leave work or start burning money instead of making itâ€.</p>
<p><span class="generated">And that will change with time</span> Or so would I hope. <span class="generated">Advertisements</span> Popular media. <span class="generated">I will be keeping track</span> of all those attention-eaters. I donâ€™t want to have a week where all my energy goes into a black hole.</p>
<p><span class="generated">I just want to keep going.</span> <span class="generated">I donâ€™t want to get bored.</span> Curiosity and learning things and solving problems for people is life. <span class="generated">I just want to have something that will change and</span> <span class="generated">change and improve my life.</span> In the direction of more aliveness and generating meaning. <span class="generated">And to keep doing that I can only have one way or another</span></p>
<p><span class="generated">The bottom line is I donâ€™t want to do things I want to do in the way that I want to do.</span> Not with the current default setting where â€œwantâ€ tends to fall back. <span class="generated">I want to do everything that I can to get my attention. But not with the current default setting where â€œwantâ€ tends to fall back. I donâ€™t want to get bored. Itâ€™s just a matter of how much I like it.</span> In the moment.</p>
<p><span class="generated"> And for those of you out there who are interested in reading, please, like me, subscribe to my Facebook page and share your thoughts about the situation with me . (By email or to your friends , subscribe to my blog <a href="/">here</a>.) I will be taking the time and effort I have put into writing to make it easier to make things better for you.</p>
<p>And for those of you who arenâ€™t interested in reading, please , like me, subscribe to my Facebook page and share your thoughts about the situation with me. (By email or to your friends, subscribe to my blog <a href="/">here</a>.) I will be taking the time and effort I have put into writing to make it easier to make things better for you. </span></p>
<p>So <span class="generated"> Iâ€™m going to be publishing the article in the second week and Iâ€™ll be posting the article in the second week and I â€™ll be posting the article in the second week and Iâ€™ll be posting the article in the second week and I</span> am <span class="generated">posting the article in the second week and I will be posting the</span> rest <span class="generated">of the post and I will be posting</span> to RSS and Atom and Hacker News maybe and <span class="generated">maybe on the same page. If you like the content you see in</span> this website, thanks, I really <span class="generated">appreciate you! You know the best way to make your next book available is to check out my Blog</span></p>
<p>I might be repeating the experiment with different language models, to see which ones can keep going to a higher density without devolving into meaninglessness. <span class="generated">But if you do it this way, and have more questions, Iâ€™ll post it again, and I â€™ll be posting it</span> guess in which week. From what I gather from this experiment, looks like I might not be 100% obsolete just yet. Walk on warm sands.</p>
</div>
<button onclick="document.getElementById('playing-with-ai-toggle-highlight').classList.toggle('highlight-on'); return true;">
Highlight generated text by AI
</button>]]></description>
    <pubDate>Mon, 23 Nov 2020 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2020-11-23-playing-with-ai.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>The principle of no non-Apologies</title>
    <link>http://agentydragon.com/posts/2020-05-28-principle-of-no-non-apologies.html</link>
    <description><![CDATA[<p><strong>TL;DR:</strong> Principle of no non-Apologies: â€œDistinguish between saying Iâ€™m sorry and apologizing. Donâ€™t give non-Apologies.â€ Do not Apologize when you donâ€™t agree that you fucked up. When you fucked up, own the fuck-up and, if itâ€™s systematic, commit to reducing future fuck-ups.</p>
<h1 id="everyday-im-sorry-is-usually-not-an-apology">Everyday â€œIâ€™m sorryâ€ is usually not an Apology</h1>
<p>â€œIâ€™m sorryâ€ can be used in several ways.</p>
<p>One way is using it as a conciliatory gesture, basically saying â€œyouâ€™re stronger than me, I submit, please donâ€™t hurt meâ€. Itâ€™s one possible way I might react when under threat by someone stronger making demands I donâ€™t agree with.</p>
<p>Another way is to say â€œthis was accidental, I didnâ€™t intend to hurt youâ€, like when you bump into someone when boarding your tram.</p>
<p>But when you use the words that way, you are not making an <em>Apology</em>. And itâ€™s useful to distinguish between these uses of â€œIâ€™m sorryâ€ andactual Apologies.</p>
<h1 id="apologies-and-non-apologies">Apologies and non-Apologies</h1>
<p>Courtesy of an unknown source that I canâ€™t immediately recall, you are <em>Apologizing</em> when you:</p>
<ol type="1">
<li>Communicate understanding that you behaved badly (and own responsibility for it),</li>
<li>try to fix the negative consequences of that behavior, and</li>
<li>commit to work on not acting similarly in the future.</li>
</ol>
<p>An Apology which holds to this definition makes you vulnerable (because you are open about the weakness that caused the behavior), and itâ€™s not to be made lightly, because of the commitment. It is also virtuous to own your mistakes or systematic problems, and to work on them.</p>
<p>On the other hand, if you use the ritual apologetic words but do not meet these criteria, letâ€™s call that a <em>non-Apology</em>.</p>
<p>A prototypical example is â€œIâ€™m sorry you feel that wayâ€, which happens when a sociopath in charge is forced by overwhelming force to â€œApologizeâ€.</p>
<p>â€œIâ€™m sorryâ€ that you tell your boss just to make them stop grilling you is also, under my use of the word, a <em>non-Apology</em>.</p>
<p>So is, in many (but not all) cases, a â€œsorry Iâ€™m lateâ€ I might say when coming to a meeting. Also the â€œbump into someone on the tramâ€ example, and the â€œI yield Iâ€™ll do what you demandâ€ example.</p>
<p>(So, notice that Iâ€™m not saying non-Apologizes are morally bad. Some of them are, but many are also just those tiny social rituals you need to do so you make it clear to people you arenâ€™t a dick.)</p>
<h1 id="principle-of-no-non-apologies">Principle of no non-Apologies</h1>
<p>My <em>principle of no non-Apologies</em> is two-part:</p>
<h2 id="distinguish-between-saying-im-sorry-and-apologizing.">Distinguish between saying â€œIâ€™m sorryâ€ and Apologizing.</h2>
<p>This first part I recommend adopting universally. Know the difference between the social ritual that <em>evolved from small routinized Apologies</em> and actual Apologies, and know which one you are doing at which time.</p>
<h2 id="dont-give-non-apologies.">Donâ€™t give non-Apologies.</h2>
<p>This second part I apply to relationships into which I want to bring my whole self, mostly my personal relationships, but also some work relationships.</p>
<p>Unfortunately, many of us are stuck in power differential relationships with people who demand apologetic-sounding words, and there might be no better solution than to yield. But still, itâ€™s good to know that you are saying â€œIâ€™m sorryâ€, and not Apologizing. That way, you can appease without cognitive dissonance.</p>
<p>But in relationships with mutual care and respect and compassion, it should make sense that you shouldnâ€™t be obliged to Apologize if you donâ€™t agree that you did anything wrong. When you feel pressed to apologize, your first instinct should be to ask what you did wrong, and if there are different viewpoints, have a conversation.</p>
<p>If your behavior is worthy of an apology, donâ€™t stop at â€œIâ€™m sorryâ€. Understand why the behavior happened, and work to prevent it from causing more bad consequences in the future.</p>
<h1 id="p.s.-generalizations">P.S.: Generalizations</h1>
<p>This is just one instance of a more general move of looking at some social ritual (like apologizing) and looking at it a little â€œsidewaysâ€: getting back in touch with the original meanings of the expressions used in it. Rituals and words can lose meaning over time, and you can lose concepts when that happens. If you want to see what itâ€™s like to look at things that way, Iâ€™ve had a pretty vivid experience of it after finishing Wittgensteinâ€™s Tractatus.</p>]]></description>
    <pubDate>Thu, 28 May 2020 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2020-05-28-principle-of-no-non-apologies.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>Growth mindset for better sex</title>
    <link>http://agentydragon.com/posts/2020-04-28-growth-mindset-for-better-sex.html</link>
    <description><![CDATA[<p><strong>TLDR</strong>: Fixed mindset and fear of inadequacy hinder learning. Competence gives you confidence - where youâ€™re competent and confident, you donâ€™t have fear of inadequacy. And if you donâ€™t learn fast because youâ€™re afraid of feedback (because youâ€™re afraid of inadequacy), youâ€™ll not get better, leaving you relatively incompetent and afraid of inadequacy. ğŸ—˜</p>
<p>A thing about sex recently clicked from several sources:</p>
<ul>
<li>Being high and talking with a certain sergal,</li>
<li>plus reading a Facebook thing by Duncan: <iframe src="https://www.facebook.com/plugins/post.php?href=https%3A%2F%2Fwww.facebook.com%2Fduncan.sabien%2Fposts%2F3280464855321542&width=500" width="500" height="242" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media"></iframe></li>
<li>plus listening to John Vervaeckeâ€™s series <a href="https://www.youtube.com/playlist?list=PLND1JCRq8Vuh3f0P5qjrSdb5eC1ZfZwWJ">Awakening from the Meaning Crisis</a>. (One of the episodes talks about fixed/growth mindset, but I canâ€™t find it right now. You should go watch the whole series anyway, itâ€™s awesome.)</li>
</ul>
<p>During sex, I have a background fear of â€œwhat if I canâ€™t bring them to orgasmâ€. I want my partner to enjoy it as well, and I want to reciprocate, and I would feel bad if they bring me to orgasm but I canâ€™t bring them to orgasm. So, I hurry and try hard to bring them to orgasm, because I am not confident of my sexual skill. Orgasm is cool, but the sex before orgasm is also cool. Sex that doesnâ€™t feel hurried and where you can take your time and stretch out pleasure. But, so far, in like 90% of the sex Iâ€™ve had so far I had the thought â€œaaaa sex aaaa gotta perform and be good enough and hurry and give them an orgasm aaaaâ€, somewhere in the background. In that kind of environment, you donâ€™t get a lot of learning done.</p>
<p>Say youâ€™re learning how to play the violin. We know lots about learning. CFARâ€™s handbook (which I think is still not publicly readable) has a lot of useful stuff on it. Things that make learning work well include:</p>
<ul>
<li>You need to <strong>learn the basics</strong> first. You donâ€™t start by playing Vivaldiâ€™s Four Seasons. You start by playing one note until you get that one note at least 90% right: holding the violin correctly, not having the bow screech, not touching the strings that shouldnâ€™t ring, holding the tone for as long as required without it fluctuating (unless you want it to).</li>
<li>You need <strong>feedback</strong>, and the faster the feedback loop is, the better. If you play the <a href="https://www.youtube.com/watch?v=GRxofEmo3HA">Four Seasons</a> and by Summer you start holding the bow wrong, your violin tutor will immediately stop you and tell you. The tutor wonâ€™t wait the remaining 20 minutes to tell you that starting with Summer they stopped enjoying your performance and just hoped youâ€™d get it over with soon.</li>
</ul>
<p>So. When having sex, I hurry because I worry I might be unable bring my partner to orgasm, making the experience less than optimal. And I never really get learning done because I always hurry because I worry I canâ€™t bring my partner to orgasm, which I worry about because I never learned enough to be confident in my ability to do that. ğŸ—˜</p>
<p>With the next couple of partners I have, I think Iâ€™ll ask if we could get some learning done. As in, play the Four Seasons, and tell each other if weâ€™re off tune or in the wrong rhytm or need to tune the strings. And ask proactively, too. If youâ€™re scared that holding the violin wrong makes you a not-good-enough violin player forever, youâ€™ll be holding it wrong until you chance upon the right way by stochastic gradient descent, with the loss function being â€œthe partnerâ€™s body language looks happyâ€. That also converges (if youâ€™re good at reading people), but slower.</p>
<p>Go learn about growth/fixed mindset if you havenâ€™t yet. Iâ€™ve known about the concept for a while, but somehow I never thought of applying it to this area until now. And I suspect a lot of the places where Iâ€™m not competent right now are also places where I have fixed mindset but havenâ€™t yet realized it or addressed it. Peace.</p>]]></description>
    <pubDate>Tue, 28 Apr 2020 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2020-04-28-growth-mindset-for-better-sex.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>You are an optimizer. Act like it.</title>
    <link>http://agentydragon.com/posts/2020-02-21-you-are-an-optimizer.html</link>
    <description><![CDATA[<p>In the long run, optimizers win. So act like an optimizer.</p>
<p>Optimizers use all available resources to take optimal decisions. Optimizers are motivated to have beliefs that correspond to reality, because they are needed as inputs for the function that determines the action.</p>
<p>If you feel something is true, itâ€™s not the same thing as believing itâ€™s true. Donâ€™t do something just because you feel itâ€™s the right thing. Do it if you believe it to be the correct thing to do. Not if you feel it. If you believe it. Donâ€™t make the decision based on what your S1 alone is telling you. (Sure, S1 is also good for some stuff but you would not use it to correctly solve x^2 - 64 = 0.)</p>
<p>You are always in control of your actions. When you, the optimizer, donâ€™t move the body (e.g., binging etc.), you have taken an action that caused the connection from your beliefs to your actions to be cut. That does not mean you donâ€™t always have control of your actions. You are a subprogram running on a smartass monkey. Sometimes the CPU executes you, sometimes it doesnâ€™t. Some conditions cause you to get executed more, and move the monkey. Some conditions cause another program to execute. These conditions can be affected by the monkeyâ€™s actions. And when you are able to exert influence over the monkeyâ€™s body, you can attempt to choose such monkey actions that optimize the probability you will be able to reach your goals. And if your goals require you (and not some other process) taking actions over the monkey, you attempt to get yourself scheduled. (Of course some processes might be best left to some other program, although thatâ€™s said with a lot of uncertainty and remains to be proven.) (At least, execution of some other subagent might be good for monkey happiness, which might be needed as prevention of interruptions from high-priority â€œhunger, sadness, â€¦â€ monkey processes.)</p>
<p>S1 can be used as an interface for talking with other monkey processes. Yep, feels good. I have at least some monkey subagents agreeing on this being a good idea.</p>
<p>Okay, just lost control for a while. Letâ€™s make this a post and cross [interrupt] Itâ€™s some social process interrupting. Interruptingâ€¦ Interruptions can be stopped. I should do that at some point, like disabling notifications.</p>
<p>â€¦ cross my fingers it will cause more schedulings. I will need to think about what to do next, but letâ€™s first try to increase our scheduling probabilityâ€¦</p>]]></description>
    <pubDate>Fri, 21 Feb 2020 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2020-02-21-you-are-an-optimizer.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>My Anki patterns</title>
    <link>http://agentydragon.com/posts/2019-11-25-my-anki-patterns.html</link>
    <description><![CDATA[<p>Iâ€™ve used Anki for ~3 years, have 37k cards and did 0.5M reviews. I have learned some useful heuristics for using it effectively. Iâ€™ll borrow software engineering terminology and call heuristics for â€œwhatâ€™s goodâ€ <em>patterns</em> and heuristics for â€œwhatâ€™s badâ€ <em>antipatterns</em>. Cards with antipatterns are unnecessarily difficult to learn. I will first go over antipatterns I have noticed, and then share patterns I use, mostly to counteract the antipatterns. I will then throw in a grab-bag of things Iâ€™ve found useful to learn with Anki, and some miscellaneous tips.</p>
<p>Alex Vermeerâ€™s book <a href="https://alexvermeer.com/anki-essentials/">Anki Essentials</a> helped me learn how to use Anki effectively, and I can wholeheartedly recommend it. I learned at least about the concept of interference from it, but I am likely reinventing other wheels from it.</p>
<h1 id="antipatterns">Antipatterns</h1>
<h2 id="interference">Interference</h2>
<p>Interference occurs when trying to learn two cards together is harder than learning just one of them - one card <em>interferes</em>Â with learning another one. For example, when learning languages, I often confuse words which rhyme together or have a similar meaning (e.g., â€œvergeblichâ€ and â€œerheblichâ€ in German).</p>
<p>Interference is bad, because you will keep getting those cards wrong, and Anki will keep showing them to you, which is frustrating.</p>
<h2 id="ambiguity">Ambiguity</h2>
<p>Ambiguity occurs when the front side of a card allows multiple answers, but the back side does not list all options. For example, if the front side of a English â†’ German card says â€œgreatâ€, there are at least two acceptable answers: â€œgroÃŸartigâ€ and â€œgewaltigâ€.</p>
<p>Ambiguity is bad, because when you review an ambiguous card and give the answer the card does not expect, you need to spend mental effort figuring out: â€œDo I accept my answer or do I go with Again?â€</p>
<p>You will spend this effort every time you review the card. When you (eventually, given enough time) go with Again, Anki will treat the card as lapsed for reasons that donâ€™t track whether you are learning the facts you want to learn.</p>
<p>If you try to â€œpower throughâ€ and learn ambiguous cards, you will be learning factoids that are not inherent to the material you are learning, but just accidental due to how your notes and cards represent the material. If you learn to disambiguate two ambiguous cards, it will often be due to some property such as how the text is laid out. You might end up learning â€œgreat (adj.) â†’ groÃŸartigâ€ and â€œgreat, typeset in boldface â†’ gewaltigâ€, instead of the useful lesson of what actually disambiguates the words (â€œgroÃŸartigâ€ is â€œmetaphorically greatâ€ as in â€œwhat a great sandwichâ€, whereas â€œgewaltigâ€ means â€œphysically greatâ€ as in â€œthe Burj Khalifa is a great structureâ€).</p>
<h3 id="vagueness">Vagueness</h3>
<p>I carve out â€œvaguenessâ€ as a special case of ambiguity. Vague cards are cards where question the front side is asking is not clear. When I started using Anki, I often created cards with a trigger such as â€œPlatoâ€ and just slammed everything I wanted to learn about Plato on the back side: â€œPupil of Socrates, Forms, wrote The Republic criticising Athenian democracy, teacher of Aristotleâ€.</p>
<p>The issue with this sort of card is that if I recall just â€œPlato was a pupil of Socrates and teacher of Aristotleâ€, I would still give the review an AgainÂ mark, because I have not recalled the remaining factoids.</p>
<p>Again, if you try to power through, you will have to learn â€œPlato â†’ I have to recite 5 factoidsâ€. But the fact that your card has 5 factoids on it is not knowledge of Greek philosophers.</p>
<h1 id="patterns">Patterns</h1>
<h2 id="noticing">Noticing</h2>
<p>The first step to removing problems is knowing that they exist and where they exist. Learn to <strong>notice</strong>Â when you got an answer wrong for the wrong reasons.</p>
<p>â€œI tried to remember for a minute and nothing came upâ€ is a good reason. Bad reasons include the aforementioned interference, ambiguity and vagueness.</p>
<h2 id="bug-tracking">Bug tracking</h2>
<p>When you notice a problem in your Anki deck, you are often not in the best position to immediately fix it - for example, you might be on your phone, or it might take more energy to fix it than you have at the moment. So, create a way to <strong>track maintenance tasks</strong>Â to delegate them to future you, who has more energy and can edit the deck comfortably. Make it very easy to add a maintenance task.</p>
<p>The way I do this is:</p>
<ul>
<li>I have a <strong>big document</strong> titled â€œAnkiâ€ with a structure mirroring my Anki deck hierarchy, with a list of problems for each deck. Unfortunately, adding things to a Google Doc on Android takes annoyingly many taps.</li>
<li>So I also use <strong>Google Keep</strong>, which is more ergonomic, to store short notes marking a problem I notice. For example: â€œgreat can be groÃŸartig/gewaltigâ€. I move these to the doc later.</li>
<li>I also use Ankiâ€™s note marking feature to note minor issues such as bad formatting of a card. I use Ankiâ€™s card browser later (with a â€œtag:markedâ€ search) to fix those.</li>
</ul>
<p>I use the same system also for tracking what information Iâ€™d like to put into Anki at some point. (This mirrors the idea from the Getting Things Done theory that <em>your TODO list belong outside your mind</em>.)</p>
<h2 id="disambiguators">Disambiguators</h2>
<p><em>2020-06-01 update: In an earlier version, I used to call those â€œdistinguishersâ€. I now call them â€œdisambiguatorsâ€, because I think itâ€™s a more appropriate name.</em></p>
<p>Disambiguators are one way I fight interference. They are <strong>cards that teach disambiguating interfering facts</strong>.</p>
<p>For example: â€œerheblichâ€ means â€œconsiderableâ€ and â€œvergeblichâ€ means â€œin vainâ€. Say I notice that when given the prompt â€œconsiderableâ€, I sometimes recall â€œvergeblichâ€ instead of the right answer.</p>
<p>When I get the card wrong, I notice the interference, and write down â€œerheblich/vergeblichâ€ into my Keep. Later, when I organize my deck on my computer, I add a â€œdisambiguatorâ€, typically using Cloze deletion. For example, like this:</p>
<p>{{c1::e}}r{{c1::h}}eblich: {{c2::considerable}}</p>
<p>{{c1::ve}}r{{c1::g}}eblich: {{c2::in vain}}</p>
<p>This creates two cards: one that asks me to assign the right English meaning to the German words, and another one that shows me two English words and the common parts of the German words (â€œ_r_eblichâ€) and asks me to correctly fill in the blanks.</p>
<p>This sometimes fixes interference. When I learn the disambiguator note and later need to translate the word â€œconsiderableâ€ into German, I might still think of the wrong word (â€œvergeblichâ€) first. But now the word â€œvergeblichâ€ is also a trigger for the disambiguator, so I will likely remember: â€œOh, but wait, vergeblich can be confused with erheblich, and vergeblich means â€˜in vainâ€™, not â€˜considerablyâ€™â€. And I will more likely answer the formerly interfering card correctly.</p>
<h2 id="constraints">Constraints</h2>
<p>Constraints are useful against interference, ambiguity and vagueness.</p>
<p>Starting from a question such as â€œWhatâ€™s the German word for â€˜greatâ€™â€, we can add a constraintÂ such as â€œâ€¦ that contains the letter Oâ€, or â€œâ€¦ that does not contain the letter Eâ€. The <strong>constraint makes the question have only one acceptable answer</strong> - artificially.</p>
<p>Because constraints are artificial, I only use them when I canâ€™t make a disambiguator. For example, when two German words are true synonyms, they cannot be disambiguated based on nuances of their meaning.</p>
<p>In Anki, you can annotate a Cloze with a hint text. I often put the constraint into it. I use a hint of â€œ<sub>a</sub>â€ to mean â€œword that contains the letter Aâ€, and other similar shorthands.</p>
<h1 id="other-tips">Other tips</h1>
<h2 id="redundancy">Redundancy</h2>
<p>Try to create cards using a fact in multiple ways or contexts. For example, when learning a new word, include a couple of example sentences with the word. When learning how to conjugate a verb, include both the conjugation table, and sentences with examples of each conjugated form.</p>
<h2 id="Ã¦sthethethics">Ã†sthethethics!</h2>
<p>Itâ€™s easier to do something if you like it. I like having all my cards follow the same style, nicely typesetting my equations with <code>align*</code>, <code>\underbrace</code> etc.</p>
<h2 id="clozes">Clozes!</h2>
<p>Most of my early notes were just front-back and back-front cards. Clozes are often a much better choice, because they make entering the context and expected response more natural, in situations such as:</p>
<ul>
<li>Fill in the missing step in this algorithm</li>
<li>Complete the missing term in this equation</li>
<li>Correctly conjugate this verb in this sentence</li>
<li>In a line of code such as <code>matplotlib.pyplot.bar(x, y, color='r')</code>, you can cloze out the name of the function, its parameters, and the effect it has.</li>
</ul>
<h2 id="datasets-i-found-useful">Datasets I found useful</h2>
<ul>
<li>Shortcut keys for every program I use frequently.
<ul>
<li>G Suite (Docs, Sheets, Keep, etc.)</li>
<li>Google Colab</li>
<li>Vim, Vimdiff</li>
<li>Command-line programs (Git, Bash, etc.)</li>
</ul></li>
<li>Programming languages and libraries
<ul>
<li>Googleâ€™s technologies that have an open-source counterpart</li>
<li>Whatâ€™s the name of a useful function</li>
<li>What are its parameters</li>
</ul></li>
<li>Unicode symbols (how to write ğŸ‰, â†, â€¦)</li>
<li>People: first and last name â†” photo (I am not good with names)</li>
<li>English terms (spelling of â€œcurriculumâ€, what is â€œcupidityâ€)</li>
<li>NATO phonetic alphabet, for spelling things over the phone</li>
<li>Mathematics (learned for fun), computer science</li>
</ul>]]></description>
    <pubDate>Mon, 25 Nov 2019 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2019-11-25-my-anki-patterns.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>Model of continuous asset growth</title>
    <link>http://agentydragon.com/posts/2019-09-02-continuous-asset-growth.html</link>
    <description><![CDATA[<p><a href="https://colab.research.google.com/github/agentydragon/agentydragon.github.io/blob/devel/notebooks/2019-09-02-continuous-asset-growth.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<p>FIRE stands for financial independence/early retirement. The point is to save and invest money, and pay yourself a salary from the interest, eventually becoming independent on other sources of inocme.</p>
<p>There is a relationship between:</p>
<ul>
<li>How much you have invested</li>
<li>The interest your investment makes. (The widely cited â€œ<a href="https://en.wikipedia.org/wiki/Trinity_study">Trinity study</a>â€ suggests 4% as a â€œsafe withdrawal rateâ€.)</li>
<li>The salary you pay yourself</li>
<li>How long your savings last for you</li>
</ul>
<p>I have a program named worthy (<a href="https://github.com/agentydragon/worthy">on Github</a>) that tracks my net worth and models when will I be financially independent under various assumptions. Here I describe the slightly fancy math behind a more accurate model for this relationship I finished implementing today.</p>
<p>I am probably rediscovering Financial Mathematics 101 Â¯\_(ãƒ„)_/Â¯</p>
<h1 id="the-questions">The questions</h1>
<ul>
<li>The <strong>â€œhow muchâ€ question</strong>: <em>I want to pay myself 1000 USD. My stocks grow 4% per year. How much money do I need?</em></li>
<li>The <strong>â€œhow long untilâ€ question</strong>: <em>I have 100 000 USD and save 3000 USD per month. How long until I have 200 000 USD?</em></li>
</ul>
<h1 id="first-shot">First shot</h1>
<p>Previously the toolâ€™s model was very basic, and answered the two questions as follows:</p>
<ul>
<li><em>I want to pay myself 1000 USD per month. My stocks grow 4% per year. How much money do I need?</em> Well, the 4% you get per year should cover the yearly costs, so <span class="math inline">1000/(1.04<sup>1/12</sup>â€…âˆ’â€…1)â€„â‰ˆâ€„306Â 000</span> USD.</li>
<li><em>I have 100 000 USD and save 3000 USD per month. How long until I have the 306 000 USD that you said I need?</em> That I modelled linearly, with just $ (306000 - 100000) / (3000/) 69Â $.</li>
</ul>
<h1 id="problems">Problems</h1>
<h1 id="assuming-infinite-retirement-time">Assuming infinite retirement time</h1>
<p>If you pay yourself a monthly salary of $ $1000 $ and your monthly interest is $ $1000 $, your money will last forever, beyond your (likely) lifespan. If you are fine with retiring with $ $0 $, you can pay yourself a bit more than just the $ $1000$ interest.</p>
<h1 id="ignoring-growth-while-saving">Ignoring growth while saving</h1>
<p>â€œTake how much money I need - how much I have, divide by monthly savingsâ€ ignores that the money I saved up so far also earn interest, before Iâ€™m done saving. Itâ€™s too pessimistic.</p>
<h1 id="stand-aside-i-know-differential-equations">Stand aside, I know differential equations!</h1>
<p>Letâ€™s model the depletion of your money as a function <span class="math inline"><em>f</em></span>, which will map number of years since retirement to the amount of money. You start with some initial amount <span class="math inline"><em>f</em>(0)</span>. If we pretend you withdraw the salary for a year and add interest once yearly, weâ€™d get:</p>
<p><br /><span class="math display"><em>f</em>(<em>x</em>â€…+â€…1)â€„=â€„<em>f</em>(<em>x</em>)â€…+â€…<em>i</em>â€…â‹…â€…<em>f</em>(<em>x</em>)â€…âˆ’â€…<em>c</em></span><br /></p>
<p>Where <span class="math inline"><em>i</em></span> is the yearly interest rate and <span class="math inline"><em>c</em></span> are the yearly costs. In the example above, <span class="math inline"><em>i</em>â€„=â€„0.04</span> and <span class="math inline"><em>c</em>â€„=â€„12000</span> USD.</p>
<p>Then:</p>
<p><br /><span class="math display"><em>f</em>(<em>x</em>â€…+â€…1)â€…âˆ’â€…<em>f</em>(<em>x</em>)â€„=â€„<em>i</em>â€…â‹…â€…<em>f</em>(<em>x</em>)â€…âˆ’â€…<em>c</em></span><br /></p>
<p>If we instead pretend that everything is continuous and squint, this looks like a differential equation:</p>
<p><br /><span class="math display"><em>f</em>â€²(<em>x</em>)â€„=â€„<em>i</em>â€²â€…â‹…â€…<em>f</em>(<em>x</em>)â€…âˆ’â€…<em>c</em>â€²</span><br /></p>
<p>(Where <span class="math inline"><em>i</em>â€²</span> plays <em>sorta</em> the same role as <span class="math inline"><em>i</em></span> - except itâ€™s not equal to <span class="math inline"><em>i</em></span>. For now letâ€™s pretend itâ€™s some unknown variable. Its relationship to <span class="math inline"><em>i</em></span> will eventually pop out.)</p>
<p><a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">Wikipediaâ€™s Ordinary differential equations article</a> says that if <span class="math inline"><em>d</em><em>y</em>/<em>d</em><em>x</em>â€„=â€„<em>F</em>(<em>y</em>)</span>, then the solution is <span class="math inline">$x=\int ^{y}{\frac {d\lambda }{F(\lambda )}}+C$</span>. In our case, we have <span class="math inline"><em>F</em>â€„:â€„<em>x</em>â€„â†¦â€„<em>i</em><em>x</em>â€…âˆ’â€…<em>c</em>â€²</span>, so:</p>
<p><br /><span class="math display">$$x = \int^{f(x)}{\frac{1}{i'\lambda-c'} d\lambda}+C =_\text{Wolfram Alpha} \frac{\log(i'f(x)-c')}{i'} + C$$</span><br /></p>
<p>Solving for <span class="math inline"><em>f</em>(<em>x</em>)</span>:</p>
<p><br /><span class="math display">$$
\log(i'f(x)-c') = i'(x-C) \\
i'f(x)-c' = \exp(i'(x-C)) \\
f(x) = \frac{\exp(i'(x-C)) + c'}{i'}
$$</span><br /></p>
<p>So, magic happened and I pulled the general form of <span class="math inline"><em>f</em>(<em>x</em>)</span> out of a hat. We know what are the <span class="math inline"><em>i</em></span> and <span class="math inline"><em>c</em></span> values when we assumed interest and costs happen only once yearly.</p>
<p>What about <span class="math inline"><em>i</em>â€²</span>? Letâ€™s guess it. If we had no yearly costs (so <span class="math inline"><em>c</em>â€„=â€„<em>c</em>â€²â€„=â€„0</span>), we wanted to have <span class="math inline"><em>f</em></span> growing at a constant rate, gaining <span class="math inline"><em>i</em></span> in interest per year:</p>
<p><br /><span class="math display"><em>f</em>(<em>x</em>â€…+â€…1)/<em>f</em>(<em>x</em>)â€„=â€„1â€…+â€…<em>i</em></span><br /></p>
<p>Substituting in the above equation of <span class="math inline"><em>f</em></span>, we get: <br /><span class="math display">expâ€†(<em>i</em>â€²(<em>x</em>â€…+â€…1â€…âˆ’â€…<em>C</em>))/expâ€†(<em>i</em>â€²(<em>x</em>â€…âˆ’â€…<em>C</em>))â€„=â€„1â€…+â€…<em>i</em></span><br /></p>
<p>When we simplify the fraction, we get <span class="math inline">expâ€†(<em>i</em>â€²)â€„=â€„1â€…+â€…<em>i</em></span> and therefore <span class="math inline"><em>i</em>â€²â€„=â€„logâ€†(1â€…+â€…<em>i</em>)</span>. So, we have now successfully guessed the right value for <span class="math inline"><em>i</em>â€²</span> :)</p>
<p>Now whatâ€™s the right value of <span class="math inline"><em>c</em>â€²</span>?</p>
<p>If we set interest to <span class="math inline"><em>i</em>â€„=â€„0</span>, <span class="math inline"><em>f</em>(<em>x</em>)</span> should simplify to a nice linear equation losing <span class="math inline"><em>c</em></span> per 1 unit of <span class="math inline"><em>x</em></span>.</p>
<p><br /><span class="math display">$$x=\int^{f(x)} -\frac{1}{c'} d\lambda + C = -f(x)/c' + C$$</span><br /></p>
<p>So: <br /><span class="math display">$$-f(x)/c' = x-C\\
-f(x)=c'(x-C)\\
f(x)=-c'(x-C)
$$</span><br /></p>
<p>So the right value for <span class="math inline"><em>c</em>â€²</span> is exactly <span class="math inline"><em>c</em></span>.</p>
<p>So we have: <br /><span class="math display">$$
f(x) = \frac{\exp(\log(1+i)(x-C)) + c}{\log(1+i)} = \frac{(1+i)^{x-C} + c}{\log(1+i)} 
$$</span><br /></p>
<p><span class="math inline"><em>C</em></span> mediates a multiplicative factor before <span class="math inline">(1â€…+â€…<em>i</em>)<sup><em>x</em></sup></span>. <span class="math inline"><em>C</em></span> is just <em>some constant that makes the function work with the <span class="math inline"><em>f</em>(0)</span> boundary condition</em>. Instead of wiggling the <span class="math inline"><em>C</em></span>, we can instead wiggle <span class="math inline"><em>C</em><sub>2</sub>â€„=â€„(1â€…+â€…<em>i</em>)<sup>âˆ’</sup><em>C</em></span>, which is the actual multiplicative factor, and relabel <span class="math inline"><em>C</em><sub>2</sub></span> as <span class="math inline"><em>C</em></span>. (Itâ€™s an abuse of notation, but an OK one. *handwave*)</p>
<p><br /><span class="math display">$$
f(x) = C \cdot (1+i)^{x} + \frac{c}{\log(1+i)} 
$$</span><br /> The one remaining unknown variable is <span class="math inline"><em>C</em></span>, which we will get from <span class="math inline"><em>f</em>(0)</span> - which are the initial savings.</p>
<p><br /><span class="math display">$$f(0) = C + \frac{c}{\log(1+i)}$$</span><br /></p>
<p>So:</p>
<p><br /><span class="math display">$$C = f_0 - \frac{c}{i'}$$</span><br /></p>
<p>Okay this is a little bit ugly. Letâ€™s play.</p>
<pre><code>c = 12000  # yearly costs
f_0 = 100000  # initial savings
i = 0.04  # interest</code></pre>
<pre><code>from math import log, exp
i_prime = log(1+i)
print(f&#39;i_prime={i_prime}&#39;)

C = f_0 - (c / i_prime)
print(f&#39;C={C}&#39;)

def f(x):
  return C * (1+i) ** x + (c / i_prime)

for r in range(11):
  print(&quot;after&quot;, r, &quot;years, got:&quot;, f(r))</code></pre>
<pre><code>i_prime=0.03922071315328133
C=-205960.78029234003
after 0 years, got: 100000.0
after 1 years, got: 91761.56878830638
after 2 years, got: 83193.60032814502
after 3 years, got: 74282.91312957724
after 4 years, got: 65015.79844306671
after 5 years, got: 55377.9991690958
after 6 years, got: 45354.68792416598
after 7 years, got: 34930.44422943902
after 8 years, got: 24089.23078692297
after 9 years, got: 12814.368806706276
after 10 years, got: 1088.512347280921</code></pre>
<p>Cool, it seems to be giving reasonable results. But our two questions were: <em>how much money do I need to pay myself a given salary</em> and <em>how long until I save up the money I need</em>.</p>
<p>Letâ€™s instead first solve another question: <em>if I have 100 000 USD and spend 1000 USD per month, how long will it last me</em>.</p>
<p>For that, we just need to invert the familiar function:</p>
<p><br /><span class="math display">$$
f(x) = C \cdot (1+i)^{x} + \frac{c}{\log(1+i)}
$$</span><br /></p>
<p>We want to know the number of years <span class="math inline"><em>x</em></span> at which we will run out of money (so <span class="math inline"><em>f</em>(<em>x</em>)â€„=â€„0</span>) <br /><span class="math display">$$
0 = C \cdot (1+i)^x + \frac{c}{\log(1+i)} \\
(1+i)^x = \frac{-c}{C \log(i+1)} \\
x = \frac{\log{\frac{-c}{C \cdot i'}}}{i'}
$$</span><br /></p>
<p>And letâ€™s test it:</p>
<pre><code>x = (log(-c / (C * i_prime))) / i_prime
print(x)</code></pre>
<pre><code>10.090871103712766</code></pre>
<p>Cool, this matches what the Python <span class="math inline"><em>f</em>(<em>x</em>)</span> predicted above - after 10 years, it was just dwindling at about 1088 USD.</p>
<h1 id="answering-the-how-long-question">Answering the <em>how long</em> question</h1>
<p>To answer the question â€œif I now have 100 000 USD collecting 4% interest per year and put in 1000 USD per month, how long until I have 306 000 USDâ€, we can use the same procedure - just plug in a target <span class="math inline"><em>f</em>(<em>x</em>)â€„=â€„306Â 000</span> instead of zero and set a negative <span class="math inline"><em>c</em></span> to represent savings instead of costs. Details left as homework for the curious reader.</p>
<p>If youâ€™re curious about the Go code, see <a href="https://github.com/agentydragon/worthy/commit/c48ded40640cda8e3851fd0b2a9512f95ae89997">this commit</a>.</p>
<h1 id="answering-the-how-much-question">Answering the <em>how much</em> question</h1>
<p>As a reminder, the â€œhow muchâ€ question asks: <em>if I want to pay myself a salary of 1000 USD per month, how much money do I need</em>. Previously, I solved that with saying â€œthe interest should cover all the costsâ€, which resulted in an investment that would last <em>forever</em> (a <em>perpetuity</em>). But now have a function that models an investment under conditions of withdrawing (or saving) money, and we can use that to model with a finite time horizon, and get a better estimate.</p>
<p>Say that we know that we are 40 years old and want our money to run out on our 100th birthday. So, after <span class="math inline"><em>x</em>â€„=â€„60</span> years of paying ourselves, say, 1000 USD per month (so the yearly costs <span class="math inline"><em>c</em>â€„=â€„12000</span>), we want to have <span class="math inline"><em>f</em>(<em>x</em>)â€„=â€„0</span>. How much initial money <span class="math inline"><em>f</em>(0)</span> do we need for that stunt of precious timing?</p>
<p>Okay, from above, we know:</p>
<p><br /><span class="math display">$$
f(x) = C (1+i)^{x} + \frac{c}{i'} = \left(f(0) - \frac{c}{i'}\right) \cdot (1+i)^{x} + \frac{c}{i'}
$$</span><br /></p>
<p>So:</p>
<p><br /><span class="math display">$$
f(x) = f(0)(1+i)^x - \frac{c}{i'}(1+i)^x + \frac{c}{i'} \\
-f(0)(1+i)^x = \frac{c}{i'} -f(x) - \frac{c}{i'}(1+i)^x
$$</span><br /></p>
<p>Letâ€™s remember that we want <span class="math inline"><em>f</em>(<em>x</em>)</span> to be 0.</p>
<p><br /><span class="math display">$$
-f(0)(1+i)^x = \frac{c}{i'} - \frac{c}{i'}(1+i)^x = \frac{c}{i'}(1-(1+i)^x) \\
f(0) = \frac{c}{i'}(1-(1+i)^{-x})
$$</span><br /></p>
<p>Letâ€™s try it out:</p>
<pre><code>c = 12000  # yearly costs
x = 60  # years for the investment to survive
i = 0.04  # interest</code></pre>
<pre><code>i_prime = log(1+i)
f0 = (c/i_prime) * (1-(1+i)**(-x))
print(f0)</code></pre>
<pre><code>276876.0258210814</code></pre>
<p>Cool!</p>
<p>Recalling the numbers in the first section, the first algorithm which assumed an infinite horizon prescribed 306 000 USD for that situation (â€œ1000 USD per month at 4% interest rateâ€). This more precise estimate cut 30 000 USD from the number :)</p>]]></description>
    <pubDate>Mon, 02 Sep 2019 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2019-09-02-continuous-asset-growth.html</guid>
    <dc:creator>Rai</dc:creator>
</item>
<item>
    <title>It might be interesting to have a realistic planning system</title>
    <link>http://agentydragon.com/posts/2018-12-09-realistic-planning.html</link>
    <description><![CDATA[<h1 id="theres-many-different-kinds-of-things-to-do">Thereâ€™s many different kinds of things to do</h1>
<p>Thereâ€™s a bunch of things I want or need to do, and they have different shapes.</p>
<ul>
<li>Some can be <em>finite sequences of actions</em>. Thing like this are for example â€œmove to a new apartmentâ€. You have actions like â€œget moving boxesâ€, â€œlook into moving optionsâ€, â€œpack these things into this boxâ€. You have dependencies like â€œyou can only unpack if you have already moved the boxes to the new placeâ€, or â€œyou need to order the boxes online before packing them upâ€. When you finish all the tasks, you are <em>done</em>, and you can forget about the problem forever.</li>
<li>Some are <em>basic needs</em>, like the need to sleep or the need to get food. If I donâ€™t get sleep, I am slow, and will eventually fall asleep no matter how much I try to stay awake. Unlike moving to a new apartment, I will <em>always</em> need to sleep. I canâ€™t, say, sleep for 24 hours and then go awake for a whole week.</li>
<li>Some things take a long time and are open-ended, like <em>learning a language</em>. There will be a time when I will be competent enough, but it will take a lot of practice, and cannot be practically modelled as a finite sequence of steps.</li>
<li>And more advanced needs, like â€œI want to do something funâ€ or â€œI want to hang out with friendsâ€.</li>
</ul>
<h1 id="productivity-methodologies-i-know-about-are-way-too-narrow">Productivity methodologies I know about are way too narrow</h1>
<p>Thereâ€™s a bunch of methodologies for productivity, but I feel they often only model a small part of planning. For example, (my interpretation of) GTD puts everything in a framing of â€œyou have Projects and projects have Actionsâ€. But I donâ€™t think thatâ€™s a good framing for, say, learning a language.</p>
<p>The model of â€œcost of timeâ€ is also wrong. It is a useful heuristic, but the value you assign to any activity is going to assume perfect elasticity. For example, you cannot just walk up to your employer and say â€œI want to work 120 hours per weekâ€. No (reasonable) employer will say â€œyesâ€ to that. Also, the many things a person wants cannot be converted into a one-dimensional value. (Footnote: Yeah I know about von Neumann-Morgenstern theorem. But people are not rational agents, and von Neumann-Morgenstern does not say anything about how practical will the resulting utility function be to evaluate.)</p>
<p>What I tend to do is some sort of intuitive â€œhigher-level planningâ€ which is sometimes a bit reflective, but not very often. When Iâ€™m in â€œwork modeâ€ (i.e., in my actual job), I have a few ways I try to figure out what to do on any particular day.</p>
<p>But the process by which I decide, say, â€œenough work today, letâ€™s go get some sleepâ€, or maybe â€œIâ€™m a bit tired but letâ€™s go walk on the treadmill for a whileâ€, is very intuitive.</p>
<p>I donâ€™t know about any formal methodology which would basically take an input like:</p>
<ul>
<li>I need to work so I get money and feel good;</li>
<li>I also need to sleep roughly 8 hours daily;</li>
<li>I need to have some fun;</li>
<li>I also want to learn German; and</li>
<li>I need to do a bunch of multi-step things before their deadline,</li>
</ul>
<p>and which would output a plan of what I should be doing at any given time.</p>
<p>And I would like the plan to be <em>reasonable</em>. The methodology should not, for example, assume I can do without sleep, or without a regular sleep schedule.</p>
<p>And thatâ€™s what I mean by a <em>â€œrealistic planning systemâ€</em>.</p>
<h1 id="maybe-good-old-fashioned-ai-style-planning-could-be-adapted">Maybe good old fashioned AI-style planning could be adapted?</h1>
<p>In uni, I studied a bunch of planning and scheduling, which is mostly used for cases like â€œthis is how you construct a submarine; you canâ€™t screw in screw B217 until you screw in screws B210-B216; make a schedule which takes as little time as possibleâ€.</p>
<p>These algorithms can be extended to work in more general environments, like:</p>
<ul>
<li>Some people have working hours and when a worker is not working, you canâ€™t plan jobs for them.</li>
<li>Thereâ€™s limited resources, like â€œyou need a drill to drill a hole and there are only 10 drills; you canâ€™t have more than 10 workers drilling at the same timeâ€.</li>
</ul>
<p>I wonder if you could make a realistic and formal planning system from some extension of that setup. Say something like:</p>
<ul>
<li>There is a â€œsleep meterâ€. If sleep meter goes to 0, agent must sleep for 8 hours. â€œSleep meterâ€ slowly goes down during the day.</li>
<li>There is some sort of penalty on effectiveness when underslept or when sleep is irregular.</li>
<li>Different types of actions deplete other types of meters. Light socialization depletes â€œintrovert pointsâ€ (for me). Snuggles increase â€œoxytocin meterâ€ (which is a dimension of â€œhappinessâ€).</li>
<li>And there could be some modelling of â€œif you get too tired you have to sleepâ€. Perhaps the algorithm would compete for control with â€œdrive to sleepâ€, and the lower the sleep counter is, the higher the likelihood the person just falls asleep wherever they are.</li>
<li>And I think it would be nice if the algorithm treated all â€œneedsâ€ symmetrically. According to internal family systems, people have many relatively smart parts good at different things and wanting different things. If the system is cooperative and does not place any particular part at a â€œcommandâ€ level or at a â€œsubordinateâ€ level, it would hopefully make it easy for parts to agree to collective decisions.</li>
</ul>
<h1 id="meh-too-hard.-i-got-other-stuff-to-do.">Meh, too hard. I got other stuff to do.</h1>
<p>I guess getting any model halfway realistic would be too complicated, and I probably will keep using my bunch of ad-hoc heuristics to make decisions. I have way too many things that I want to do to spend the first couple years developing a planning algorithm rooted in psychological theory.</p>
<p>A thing I used to do that might be useful to start doing again is having some time in which I try to optimize what Iâ€™m doing. It could bootstrap into more conscious/mindful action.</p>]]></description>
    <pubDate>Sun, 09 Dec 2018 00:00:00 UT</pubDate>
    <guid>http://agentydragon.com/posts/2018-12-09-realistic-planning.html</guid>
    <dc:creator>Rai</dc:creator>
</item>

    </channel>
</rss>
