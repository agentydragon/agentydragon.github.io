<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<title>~agentydragon/Cartpole Q-learning</title>
		<link href="http://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet" type="text/css">
		<link href="http://fonts.googleapis.com/css?family=PT+Mono" rel="stylesheet" type="text/css">
		<link rel="stylesheet" type="text/css" href="../css/default.css">
		<link rel="stylesheet" type="text/css" href="../css/syntax.css">
		<link rel="favourite icon" type="image/png" href="../images/favicon.png">
		<link rel="alternate" type="application/rss+xml" title="RSS feed for agentydragon.com" href="../rss.xml">
		<link rel="alternate" type="application/atom+xml" title="Atom feed for agentydragon.com" href="../atom.xml">
		<!-- Dollars below doubled in template because of Hakyll. -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<!--
		<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
		    extensions: ["tex2jax.js"],
		    jax: ["input/TeX", "output/HTML-CSS"],
		    tex2jax: {
		      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		      processEscapes: true
		    },
		    "HTML-CSS": { fonts: ["TeX"] }
		  });
		</script>
		-->
	</head>
	<body>
		<!--
		  Copy of #fixed_header, just takes up horizontal space to keep
		  page content in place when scrolling.
		-->
		<header>
			<a href="../" class="logo">~agentydragon/</a>

			<a href="../">Home</a>
			<a href="../archive.html">Archive</a>
			<a href="https://trilium.agentydragon.com/share/">Trilium Notes</a>
			<a href="../about.html">About</a>
		</header>

		<header id="fixed_header">
			<a href="../" class="logo">~agentydragon/</a>

			<a href="../">Home</a>
			<a href="../archive.html">Archive</a>
			<a href="https://trilium.agentydragon.com/share/">Trilium Notes</a>
			<a href="../about.html">About</a>
		</header>

		<div id="content">
			<h1>Cartpole Q-learning</h1>

			<div id="articles">
			<div class="info">
    Posted on 2020-12-31
</div>

<p>Recently I‚Äôve been working on skilling up on reinforcement learning, particularly practice. I‚Äôm currently on the last course of the <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning specialization</a> from University of Alberta on Coursera. The last piece of the course is about solving the <a href="https://gym.openai.com/envs/LunarLander-v2/">Lunar Lander</a> environment. I‚Äôve been trying to solve it on my own first before going through the labs, so that I can learn things deeper and experiment.</p>
<p>I‚Äôve tried implementing an actor-critic agent. The actor is a feed-forward neural network specifying a parameterized policy <span class="math inline"><em>œÄ</em><sub><em>Œ∏</em></sub></span>. The network‚Äôs input is a representation of the state, and it has one output per action. The policy is a softmax over these outputs. I tried a critic for predicting both <span class="math inline"><em>vÃÇ</em><sub><em>w</em></sub></span>, and <span class="math inline"><em>qÃÇ</em><sub><em>w</em></sub></span>.</p>
<p>I‚Äôve not had good luck getting this to work so far. At one point I got the agent to fly above the surface (without landing), but then later I edited the code somewhat, aaaaand it was gone.</p>
<p>I stared a bunch into my update equations, but have not been able to find any obvious errors. I used TensorFlow‚Äôs <a href="https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams">HParams</a> to try to tune all the hyperparameters, like actor learning rate, critic learning rate, and learning rate for the average reward. (I wrote it attempting to use the continuing average reward formulation.)</p>
<p>I decided to first try a simpler environment, <a href="https://gym.openai.com/envs/CartPole-v0/">CartPole</a>.</p>
<p>In the end, I managed to solve it a couple hours back.</p>
<p>In the implementation, I‚Äôve made a couple mistakes and observations, which I want to note down.</p>
<h2 id="colab-notebook-with-my-cartpole-agent">Colab notebook with my CartPole agent</h2>
<p>Here‚Äôs my notebook if you want to play around:</p>
<p><a href="https://colab.research.google.com/github/agentydragon/agentydragon.github.io/blob/devel/notebooks/2020-12-31-cartpole-q-learning.ipynb" target="_parent"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<h2 id="rais-ml-mistake-1-short-episodic-environments-can-use-high-Œ≥">Rai‚Äôs ML mistake #1: Short episodic environments can use high Œ≥</h2>
<p>I initially wrote my code to use a discount rate of 0.9. On <a href="https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/">n1try‚Äôs solution</a> that I found on <a href="https://gym.openai.com/envs/CartPole-v0/">the leaderboard</a> (unfortunately not sure which one it was), the discount rate was actually set to 1.</p>
<p>I suspect I might have set the discount rate too low. The CartPole environment has episodes which have length of only up to ~500, with 1 unit of reward per step.</p>
<p>If you have a discount rate Œ≥ and an average per-step reward of <span class="math inline"><em>r</em></span>, then in an infinite environment, the value of a state will be something like: <br /><span class="math display">$$ \frac{r}{1-\gamma} = r + r \cdot \gamma + r \cdot \gamma^2 $$</span><br /> Knowing this, I was a worried that if I set <span class="math inline"><em>Œ≥</em></span> too high, the targets for the Q network to learn would have a high variance. But I forgot that the environment had only like ~500 steps, so setting <span class="math inline"><em>Œ≥</em>‚ÄÑ=‚ÄÑ1</span> would be alright in this case.</p>
<p><em>Lesson learned</em>: I need to keep in mind the environment‚Äôs characteristics, in particular how long are the episodes and how high total rewards can I expect.</p>
<h2 id="rais-ml-mistake-2-too-little-exploration">Rai‚Äôs ML mistake #2: Too little exploration</h2>
<p>The algorithm that ended up working for me was Q-learning (with function approximation by a small neural network). I selected actions <span class="math inline"><em>Œµ</em></span>-greedily, with <span class="math inline"><em>Œµ</em></span> set to 0.02, so ~2% chance of random moves.</p>
<p>Looking at some solutions of the environment that I found, they had much higher exploration rates. Some that I saw had 100% random actions initially, and had it then decay. And the particular solution I was looking at set the minimal exploration rate, after all the decay, to <em>10%</em> - 5x more than I had.</p>
<p>I think my code found a policy that ‚Äúsolves‚Äù the environment faster when I put in 10% exploration.</p>
<h2 id="rais-ml-mistake-3-evaluation-interfering-with-training">Rai‚Äôs ML mistake #3: Evaluation interfering with training</h2>
<p>I ran my algorithm on 10k episodes, and every 1k episodes, I ran the code in ‚Äúgreedy mode‚Äù (i.e., no random actions) and recorded average performance on 100 episodes. I did that because my Q-learning implementation was executing an <span class="math inline"><em>Œµ</em></span>-soft policy, which might be worse than the greedy policy that it‚Äôs learning. I don‚Äôt know how ‚Äúfragile‚Äù the CartPole environment is (i.e., how much worse the total reward per episode gets if I force an agent to take some amount of random actions), but I wanted to rule it out as a source of errors.</p>
<p>I implemented the evaluation by just adding a flag <code>train: bool = True</code> to the agent‚Äôs functions. If <code>train</code> was <code>False</code>, I‚Äôd skip all the update steps and select actions greedily.</p>
<p>Unfortunately, I ended up making a mistake and I forgot to add the condition around one branch of code - updating the Q network after a final step (i.e., when the agent receives a last reward and the episode ends).</p>
<p>As a result, the agent ended up executing ~100 incorrect updates (based on an incorrect last action and state, towards the final reward), one incorrect update per evaluation episode.</p>
<p><strong>Lesson learned</strong>: Double check my code? Maybe even test my code? Not sure how to learn from this :/</p>
<h2 id="forgetting-when-learned-too-much"><em>Forgetting when learned too much‚ÄΩ‚ÄΩ</em></h2>
<figure>
<img src="../static/2020-12-31-total_reward.svg" style="height: 400px;" title="Total reward per episode graph">
<div>
<p><a href="https://www.youtube.com/watch?v=sIlNIVXpIns"><em>ùÑû ‚ô´ Look at this graaaph‚ô´ ùÑª</em></a></p>
</div>
</figure>
<p>So, until episode maybe 400 or so, nothing much is happening. Then until about step 1800, it‚Äôs sorta doing something but could be better. Then at around step 1800, it finds a good policy, and returns are nice. Basically, problem solved.</p>
<p>Then I train it for a bit longer, and at episode ~2.2k‚Ä¶ for some reason performance goes WAY down for about 300 or so episodes. It‚Äôs as bad as if the network forgot everything it learned before.</p>
<p>Then, after a while (at about 2700), it quickly climbs back up to good performance. On the full graph with 10k episodes, this cycle would repeat maybe every 2000-3000 episodes. Wat.</p>
<p>I have no idea what‚Äôs going on here. Maybe one sort-of ideas, but I have not tested it, and I have a relatively low degree of confidence in it.</p>
<p>Maybe for some reason the critic might overfit in some way that makes it behave badly on some early action. Maybe it‚Äôs trying to learn some relatively ‚Äúlate game‚Äù stuff, and the update that happens ends up screwing some early behavior, so the agent then has to spend a bunch of episodes learning the right early behavior again. The policy changes in a non-continuous way, so if some early decision switches to do the wrong thing, the agent will then have to follow a bunch of wrong trajectories until the one-step Q updates end up bubbling up to the initial wrong decision. I guess this might be somewhat mitigated by using eligibility traces so that updates bubble up faster, or by using actor-critic with soft policies.</p>
<p>Another potential reason for having a really bad episode might be if the agent happens to pick a random action (with probability <span class="math inline"><em>Œµ</em></span>) at an early point where the pole is unstable and very easy to mess up. And then it can‚Äôt recover from that error. But that explanation isn‚Äôt supported by how wide these areas of low episode returns are. It might explain maybe one sporadic bad episode, but not a whole bunch of them after each other.</p>
<h2 id="next-steps">Next steps</h2>
<p>Now that I got a CartPole agent running, I‚Äôll come back to the Lunar Lander environment. I‚Äôll first try solving it again with a Q network. I could probably similarly get away with not discounting rewards at all (<span class="math inline"><em>Œ≥</em>‚ÄÑ=‚ÄÑ1</span>).</p>
<p>Also I‚Äôd like to implement experience replay to make this more sample-efficient.</p>
<p>If that ends up working, I still want to get actor-critic working.</p>
<p>Obligatory scoot-scoot:</p>
<video controls loop autoplay>
<source src="../static/2020-12-31-cartpole.mp4" type="video/mp4">
</video>

			</div>
			<hr class="clearfix">
		</div>

		<footer>
			<p>
				The <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a>
				license applies unless otherwise specified.
			</p>

			<p>
				Site proudly generated by
				<a href="http://jaspervdj.be/hakyll">Hakyll</a>.
			</p>
		</footer>

		<!-- Google Analytics tracking -->
		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-17386731-5', 'auto');
			ga('send', 'pageview');
		</script>
	</body>
</html>
