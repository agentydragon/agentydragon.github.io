<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<title>~agentydragon/Knowledge base completion</title>
		<link href="http://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet" type="text/css">
		<link href="http://fonts.googleapis.com/css?family=PT+Mono" rel="stylesheet" type="text/css">
		<link rel="stylesheet" type="text/css" href="../css/default.css">
		<link rel="stylesheet" type="text/css" href="../css/syntax.css">
		<link rel="favourite icon" type="image/png" href="../images/favicon.png">
		<link rel="alternate" type="application/rss+xml" title="RSS feed for agentydragon.com" href="../rss.xml">
		<link rel="alternate" type="application/atom+xml" title="Atom feed for agentydragon.com" href="../atom.xml">
		<!-- Dollars below doubled in template because of Hakyll. -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<!--
		<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
		    extensions: ["tex2jax.js"],
		    jax: ["input/TeX", "output/HTML-CSS"],
		    tex2jax: {
		      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		      processEscapes: true
		    },
		    "HTML-CSS": { fonts: ["TeX"] }
		  });
		</script>
		-->
	</head>
	<body>
		<!--
		  Copy of #fixed_header, just takes up horizontal space to keep
		  page content in place when scrolling.
		-->
		<header>
			<a href="../" class="logo">~agentydragon/</a>

			<a href="../">Home</a>
			<a href="../archive.html">Archive</a>
			<a href="https://trilium.agentydragon.com/share/">Trilium Notes</a>
			<a href="../about.html">About</a>
		</header>

		<header id="fixed_header">
			<a href="../" class="logo">~agentydragon/</a>

			<a href="../">Home</a>
			<a href="../archive.html">Archive</a>
			<a href="https://trilium.agentydragon.com/share/">Trilium Notes</a>
			<a href="../about.html">About</a>
		</header>

		<div id="content">
			<h1>Knowledge base completion</h1>

			<div id="articles">
			<div class="info">
    Posted on 2016-10-05
</div>

<p><em>This blog post is mirrored on <a href="https://eclubprague.com/blog/knowledge-base-completion/">on eClub’s
blog</a>.</em></p>
<p>My name is Michal and I came to <a href="https://eclubprague.com/">eClub Prague</a> to work on an awesome master’s thesis.
I am interested in AI and ML applications and I sought the mentorship of
<a href="http://pasky.or.cz/">Petr Baudiš aka Pasky</a>. The project we settled on for me is
researching <em>knowledge base completion</em>.</p>
<p>You may already know something about knowledge bases. <em>Knowledge bases</em>, also known as
<em>knowledge graphs</em>, are basically knowledge represented as graphs: vertices are
entities (e.g., <code>Patricia Churchland</code>, <code>neurophilosophy</code>,
<code>University of Oxford</code>) and edges are relations between
entities (e.g., <code>Patricia Churchland <strong>studied at</strong> University of
Oxford</code>).</p>
<figure>
<img src="../static/2016-10-05-kg.png">
<div>
A small neighbourhood of <code>Patricia Churchland</code> in a hypothetical
knowledge base
</div>
</figure>
<p>Knowledge graphs are useful<sup>[citation needed]</sup>.
The most famous knowledge graph is Google’s eponymous <em>Knowledge Graph</em>.
It’s used whenever you ask Google a question like “<em>Which school did Patricia
Churchland go to?</em>”, or maybe “<em>patricia churchland alma mater</em>” – the
question is parsed into a query on the graph: “Find a node named Patricia
Churchland, find all edges going from that node labeled <strong>studied at</strong>, and
print the labels of nodes they point at.” Look:</p>
<figure>
<img src="../static/2016-10-05-churchland.png">
<div>
How Google uses knowledge bases
</div>
</figure>
<p><a href="https://en.wikipedia.org/wiki/Knowledge_Graph">Google’s Knowledge Graph</a> is based on
<a href="https://developers.google.com/freebase/">Freebase</a> (<a href="https://en.wikipedia.org/wiki/Freebase">wiki</a>), which is now frozen,
but its data is still publicly available. Other knowledge bases include <a href="http://wiki.dbpedia.org/">DBpedia</a>,
which is created by automatically parsing Wikipedia articles, and <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a>,
which is maintained by an army of volunteers with too much free time on their
hands.</p>
<p>Because knowledge graphs are useful, we would like them to contain all true
facts about the world, but the world is big<sup>[citation needed]</sup>.
And since we want to represent everything in knowledge graphs, we’d need
a really big number of editors.
Real-life knowledge graphs miss a lot of facts. Persons might not have their
nationalities assigned, cities might be missing their population numbers, and
so on.</p>
<p>Someone had the bright idea that maybe we could replace some of the editing
work by AI, and indeed, so we can! When we try to add missing true facts into
a partially populated knowledge base, we are doing <em>knowledge base completion</em>.</p>
<p>Researchers have thrown many wildly different ideas at the problem, and some of
them stuck. For example:</p>
<ul>
<li><em>Extracting relations from unstructured text.</em> This means I take a knowledge
base, some piece of text (I’m using Wikipedia articles), and I try to
fill the gaps using the text.
<br>
The canonical approach is to train a classifier for each relation type,
for example the relation <strong>actor played in movie</strong>. The classifier
takes a sentence as its input, and outputs a number between 0 and 1, which
represents the classifier’s estimate on the likelihood this sentence
represents the relation. So, on sentences like <code>University of Oxford
is in Oxford, UK.</code> or <code>Marie Curie died of radium poisoning.</code>,
we’d like to see low scores. On the other hand, sentences like
<code>Arnold Schwarzenegger played in the movie Terminator.</code>
should score close to 1.
<br>
A problem with this approach is that we’d need a really large training set
to train our classifier well, and who has the time to label 10k+ sentences
those days? Fortunately, we can use a neat trick called <em>distant
supervision</em>. Interested? Read about it in <a href="https://web.stanford.edu/~jurafsky/mintz.pdf">this paper</a>.</li>
<li><em>Mining for graph patterns.</em> In real life, we know that if Peter is the
father of John and Kate is John’s mother, it’s pretty likely that Peter and
Kate might be married. So, if our knowledge base contains the facts
<code>Peter is the father of John</code> and <code>Kate is the mother of
John</code>, then if the knowledge base doesn’t say that <code>Peter is
married to Kate</code>, we could expect that to be an error of omission.
On the other hand, if we add the fact that <code>Peter is married to
Marie</code>, that counts as evidence against Peter being married to Kate.
<br>
There are algorithms that look for such patterns (and more complex ones)
in the incomplete knowledge graph and then use these patterns on the same
graph to assign likelihoods to missing relations.
One is called PRA (<em>Path Ranking Algorithm</em>), another one SFE (<em>Subgraph
Feature Extraction</em>). Matt Gardner has <a href="https://matt-gardner.github.io/pra/">an implementation of
both</a>.</li>
<li><em>Embeddings.</em> This means that we invent a space with, say, 50 dimensions,
and somehow represent entities and relations within that space. We choose
this representation so that the embedding then informs us about which
relations might be true, but missing in the knowledge graph.
<br>
For example, say we represent people as points in a 2D plane, and we
represent the <code>is mother of</code> relation as a step to the right and
up, and the <code>is father of</code> relation as a step to the right and
down. Of course, you can’t really represent all the complexity of family
relationships this way, but if you tried to get as close as you could,
you’d end up with a figure where two people who are siblings would
probably end up <em>close together</em>. Behold: embedding according to <code>is
mother of</code> and <code>is father of</code> just told us something about
who <code>is sibling of</code> who!</li>
</ul>
<p>In my thesis, I plan to replicate the architecture of Google’s Knowledge Vault
(<a href="https://www.cs.ubc.ca/~murphyk/Papers/kv-kdd14.pdf">paper</a>, <a href="https://en.wikipedia.org/wiki/Knowledge_Vault">wiki</a>).</p>
<p>Google created Knowledge Vault, because they wanted to build a knowledge graph
even bigger than <em>The</em> Knowledge Graph.</p>
<p>The construction of Knowledge Vault takes Knowledge Graph as its input, and
uses three different algorithms to infer probabilities for new relations.
One of these algorithms <em>extracts new relations from webpages</em>. The second one
uses PRA to predict new edges from graph patterns. The third one learns an
embedding of Knowledge Graph and predicts new relations from this embedding.</p>
<p>Each of these different approaches yields its own probability estimates
for new facts.
The final step is training a new classifier that takes these estimates and
merges them into one unified probability estimate.</p>
<p>Finally, you take all the predicted relations and their probability estimates,
you store them, and you have your own Knowledge Vault. Unlike the input
knowledge graph, this output is probabilistic: for each <code>Subject</code>,
<code>Relation</code>, <code>Object</code> triple, we also store the estimated
probability of that triple being true. The output is much larger than the input
graph, because it needs to store many edges that weren’t in the original
knowledge graph.</p>
<p>Why is this useful? Because the indiviual algorithms (extraction from text,
graph pattern mining and embeddings) have complementary
strengths and weaknesses, so combining them gets you a system that can
predict more facts.</p>
<figure>
<img src="../static/2016-10-05-knowledge-vault.png">
<div>
Simplified Knowledge Vault architecture
</div>
</figure>
<p>My system is open-source and extends Wikidata. The repository
is at <a href="https://github.com/MichalPokorny/master" class="uri">https://github.com/MichalPokorny/master</a>.</p>
<p>So far, I have been setting up my infrastructure. A week back, I finally
got the first version of my pipeline, with the stupidest algorithm and
the smallest data I could use, running and predicting end-to-end!</p>
<p>The system generated 116 predictions with an estimated probability higher than
0.5. Samples include:</p>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 24%" />
<col style="width: 16%" />
<col style="width: 11%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>Subject</th>
<th>Relation</th>
<th>Object</th>
<th>Probability</th>
<th>Is this fact true?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Northumberland</td>
<td>occupation</td>
<td>Film director</td>
<td>0.5913</td>
<td>False</td>
</tr>
<tr class="even">
<td>mathematician</td>
<td>occupation</td>
<td>Film director</td>
<td>0.6201</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Jacob Zuma</td>
<td>member of political party</td>
<td>Zulu people</td>
<td>0.5159</td>
<td>False</td>
</tr>
<tr class="even">
<td>Mehmed VI</td>
<td>country of citizenship</td>
<td>Ottoman Empire</td>
<td>0.5479</td>
<td>True</td>
</tr>
<tr class="odd">
<td>Brian Baker</td>
<td>country of citizenship</td>
<td>Australia</td>
<td>0.5523</td>
<td>False</td>
</tr>
<tr class="even">
<td>swimming</td>
<td>occupation</td>
<td>Film director</td>
<td>0.6229</td>
<td>False <!-- ? --></td>
</tr>
<tr class="odd">
<td>West Virginia</td>
<td>member of political party</td>
<td>Tennessee</td>
<td>0.5107</td>
<td>False <!-- ? --></td>
</tr>
<tr class="even">
<td>Tamim bin Hamad Al Thani</td>
<td>country of citizenship</td>
<td>Princely Family of Liechtenstein</td>
<td>0.5289</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Sheldon Whitehouse</td>
<td>member of political party</td>
<td>Washington, D.C.</td>
<td>0.5086</td>
<td>False</td>
</tr>
<tr class="even">
<td>Liberation Tigers of Tamil Eelam</td>
<td>country of citizenship</td>
<td>United States</td>
<td>0.5349</td>
<td>False <!-- ? --></td>
</tr>
<tr class="odd">
<td>Italian Communist Party</td>
<td>country of citizenship</td>
<td>United States</td>
<td>0.5545</td>
<td>False <!-- ? --></td>
</tr>
<tr class="even">
<td>Henri Matisse</td>
<td>country of citizenship</td>
<td>French</td>
<td>0.5004</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Lawrence Ferlinghetti</td>
<td>country of citizenship</td>
<td>United States</td>
<td>0.5471</td>
<td>True</td>
</tr>
<tr class="even">
<td>Michael Andersson</td>
<td>occupation</td>
<td>Film director</td>
<td>0.6283</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Brian Baker</td>
<td>country of citizenship</td>
<td>Canada</td>
<td>0.5187</td>
<td>False</td>
</tr>
<tr class="even">
<td>John E. Sweeney</td>
<td>member of political party</td>
<td>Republican Party</td>
<td>0.5036</td>
<td>True</td>
</tr>
</tbody>
</table>
</table>
<p>Okay sooo… not <em>super</em> impressive, but pretty good for a first shot.
At least it does a bit better than rolling a bunch of dice :)</p>
<p>It’s basically a logistic regression over a bag of words. The dataset
are 10 000 Wikipedia articles about persons.
My task now is to use smarter algorithms to get better results,
adding other algorithms (graph patterns and embeddings), running over
a larger dataset and fleshing out the architecture.</p>
<p>I’d enjoy talking at length about the various design choices and cool tools
I’m using, but I was told 1 A4 page would be quite enough, so I’ll cut my
proselytization short just about now.</p>
<p>Let’s get back to work.
Have a fine day, and may your values be optimally satisfied!</p>

			</div>
			<hr class="clearfix">
		</div>

		<footer>
			<p>
				The <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a>
				license applies unless otherwise specified.
			</p>

			<p>
				Site proudly generated by
				<a href="http://jaspervdj.be/hakyll">Hakyll</a>.
			</p>
		</footer>

		<!-- Google Analytics tracking -->
		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-17386731-5', 'auto');
			ga('send', 'pageview');
		</script>
	</body>
</html>
