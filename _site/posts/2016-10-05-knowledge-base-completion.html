<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<title>~agentydragon/Knowledge base completion</title>
		<link href="http://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet" type="text/css">
		<link href="http://fonts.googleapis.com/css?family=PT+Mono" rel="stylesheet" type="text/css">
		<link rel="stylesheet" type="text/css" href="../css/default.css">
		<link rel="stylesheet" type="text/css" href="../css/syntax.css">
		<link rel="favourite icon" type="image/png" href="../images/favicon.png">
		<link rel="alternate" type="application/rss+xml" title="RSS feed for agentydragon.com" href="../rss.xml">
		<link rel="alternate" type="application/atom+xml" title="Atom feed for agentydragon.com" href="../atom.xml">
		<!-- Dollars below doubled in template because of Hakyll. -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<!--
		<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
		    extensions: ["tex2jax.js"],
		    jax: ["input/TeX", "output/HTML-CSS"],
		    tex2jax: {
		      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		      processEscapes: true
		    },
		    "HTML-CSS": { fonts: ["TeX"] }
		  });
		</script>
		-->
	</head>
	<body>
		<!-- Just keeps content in place. -->
		<header>
			<a href="../" class="logo">~agentydragon/</a>

			<a href="../">Home</a>
			<a href="../archive.html">Archive</a>
			<a href="../wiki/index.html">Wiki</a>
			<a href="../about.html">About</a>
		</header>

		<header id="fixed_header">
			<a href="../" class="logo">~agentydragon/</a>

			<a href="../">Home</a>
			<a href="../archive.html">Archive</a>
			<a href="../wiki/index.html">Wiki</a>
			<a href="../about.html">About</a>
		</header>

		<div id="content">
			<h1>Knowledge base completion</h1>

			<div id="articles">
			<div class="info">
    Posted on 2016-10-05
</div>

<p><em>This blog post is mirrored on <a href="https://eclubprague.com/blog/knowledge-base-completion/">on eClub’s blog</a>.</em></p>
<p>My name is Michal and I came to <a href="https://eclubprague.com/">eClub Prague</a> to work on an awesome master’s thesis. I am interested in AI and ML applications and I sought the mentorship of <a href="http://pasky.or.cz/">Petr Baudiš aka Pasky</a>. The project we settled on for me is researching <em>knowledge base completion</em>.</p>
<p>You may already know something about knowledge bases. <em>Knowledge bases</em>, also known as <em>knowledge graphs</em>, are basically knowledge represented as graphs: vertices are entities (e.g., <code>Patricia Churchland</code>, <code>neurophilosophy</code>, <code>University of Oxford</code>) and edges are relations between entities (e.g., <code>Patricia Churchland <strong>studied at</strong> University of Oxford</code>).</p>
<figure>
<img src="../static/2016-10-05-kg.png">
<div>
A small neighbourhood of <code>Patricia Churchland</code> in a hypothetical knowledge base
</div>
</figure>
<p>Knowledge graphs are useful<sup>[citation needed]</sup>. The most famous knowledge graph is Google’s eponymous <em>Knowledge Graph</em>. It’s used whenever you ask Google a question like “<em>Which school did Patricia Churchland go to?</em>”, or maybe “<em>patricia churchland alma mater</em>” – the question is parsed into a query on the graph: “Find a node named Patricia Churchland, find all edges going from that node labeled <strong>studied at</strong>, and print the labels of nodes they point at.” Look:</p>
<figure>
<img src="../static/2016-10-05-churchland.png">
<div>
How Google uses knowledge bases
</div>
</figure>
<p><a href="https://en.wikipedia.org/wiki/Knowledge_Graph">Google’s Knowledge Graph</a> is based on <a href="https://developers.google.com/freebase/">Freebase</a> (<a href="https://en.wikipedia.org/wiki/Freebase">wiki</a>), which is now frozen, but its data is still publicly available. Other knowledge bases include <a href="http://wiki.dbpedia.org/">DBpedia</a>, which is created by automatically parsing Wikipedia articles, and <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a>, which is maintained by an army of volunteers with too much free time on their hands.</p>
<p>Because knowledge graphs are useful, we would like them to contain all true facts about the world, but the world is big<sup>[citation needed]</sup>. And since we want to represent everything in knowledge graphs, we’d need a really big number of editors. Real-life knowledge graphs miss a lot of facts. Persons might not have their nationalities assigned, cities might be missing their population numbers, and so on.</p>
<p>Someone had the bright idea that maybe we could replace some of the editing work by AI, and indeed, so we can! When we try to add missing true facts into a partially populated knowledge base, we are doing <em>knowledge base completion</em>.</p>
<p>Researchers have thrown many wildly different ideas at the problem, and some of them stuck. For example:</p>
<ul>
<li><em>Extracting relations from unstructured text.</em> This means I take a knowledge base, some piece of text (I’m using Wikipedia articles), and I try to fill the gaps using the text. <br> The canonical approach is to train a classifier for each relation type, for example the relation <strong>actor played in movie</strong>. The classifier takes a sentence as its input, and outputs a number between 0 and 1, which represents the classifier’s estimate on the likelihood this sentence represents the relation. So, on sentences like <code>University of Oxford is in Oxford, UK.</code> or <code>Marie Curie died of radium poisoning.</code>, we’d like to see low scores. On the other hand, sentences like <code>Arnold Schwarzenegger played in the movie Terminator.</code> should score close to 1. <br> A problem with this approach is that we’d need a really large training set to train our classifier well, and who has the time to label 10k+ sentences those days? Fortunately, we can use a neat trick called <em>distant supervision</em>. Interested? Read about it in <a href="https://web.stanford.edu/~jurafsky/mintz.pdf">this paper</a>.</li>
<li><em>Mining for graph patterns.</em> In real life, we know that if Peter is the father of John and Kate is John’s mother, it’s pretty likely that Peter and Kate might be married. So, if our knowledge base contains the facts <code>Peter is the father of John</code> and <code>Kate is the mother of John</code>, then if the knowledge base doesn’t say that <code>Peter is married to Kate</code>, we could expect that to be an error of omission. On the other hand, if we add the fact that <code>Peter is married to Marie</code>, that counts as evidence against Peter being married to Kate. <br> There are algorithms that look for such patterns (and more complex ones) in the incomplete knowledge graph and then use these patterns on the same graph to assign likelihoods to missing relations. One is called PRA (<em>Path Ranking Algorithm</em>), another one SFE (<em>Subgraph Feature Extraction</em>). Matt Gardner has <a href="https://matt-gardner.github.io/pra/">an implementation of both</a>.</li>
<li><em>Embeddings.</em> This means that we invent a space with, say, 50 dimensions, and somehow represent entities and relations within that space. We choose this representation so that the embedding then informs us about which relations might be true, but missing in the knowledge graph. <br> For example, say we represent people as points in a 2D plane, and we represent the <code>is mother of</code> relation as a step to the right and up, and the <code>is father of</code> relation as a step to the right and down. Of course, you can’t really represent all the complexity of family relationships this way, but if you tried to get as close as you could, you’d end up with a figure where two people who are siblings would probably end up <em>close together</em>. Behold: embedding according to <code>is mother of</code> and <code>is father of</code> just told us something about who <code>is sibling of</code> who!</li>
</ul>
<p>In my thesis, I plan to replicate the architecture of Google’s Knowledge Vault (<a href="https://www.cs.ubc.ca/~murphyk/Papers/kv-kdd14.pdf">paper</a>, <a href="https://en.wikipedia.org/wiki/Knowledge_Vault">wiki</a>).</p>
<p>Google created Knowledge Vault, because they wanted to build a knowledge graph even bigger than <em>The</em> Knowledge Graph.</p>
<p>The construction of Knowledge Vault takes Knowledge Graph as its input, and uses three different algorithms to infer probabilities for new relations. One of these algorithms <em>extracts new relations from webpages</em>. The second one uses PRA to predict new edges from graph patterns. The third one learns an embedding of Knowledge Graph and predicts new relations from this embedding.</p>
<p>Each of these different approaches yields its own probability estimates for new facts. The final step is training a new classifier that takes these estimates and merges them into one unified probability estimate.</p>
<p>Finally, you take all the predicted relations and their probability estimates, you store them, and you have your own Knowledge Vault. Unlike the input knowledge graph, this output is probabilistic: for each <code>Subject</code>, <code>Relation</code>, <code>Object</code> triple, we also store the estimated probability of that triple being true. The output is much larger than the input graph, because it needs to store many edges that weren’t in the original knowledge graph.</p>
<p>Why is this useful? Because the indiviual algorithms (extraction from text, graph pattern mining and embeddings) have complementary strengths and weaknesses, so combining them gets you a system that can predict more facts.</p>
<figure>
<img src="../static/2016-10-05-knowledge-vault.png">
<div>
Simplified Knowledge Vault architecture
</div>
</figure>
<p>My system is open-source and extends Wikidata. The repository is at <a href="https://github.com/MichalPokorny/master" class="uri">https://github.com/MichalPokorny/master</a>.</p>
<p>So far, I have been setting up my infrastructure. A week back, I finally got the first version of my pipeline, with the stupidest algorithm and the smallest data I could use, running and predicting end-to-end!</p>
<p>The system generated 116 predictions with an estimated probability higher than 0.5. Samples include:</p>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 24%" />
<col style="width: 16%" />
<col style="width: 11%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>Subject</th>
<th>Relation</th>
<th>Object</th>
<th>Probability</th>
<th>Is this fact true?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Northumberland</td>
<td>occupation</td>
<td>Film director</td>
<td>0.5913</td>
<td>False</td>
</tr>
<tr class="even">
<td>mathematician</td>
<td>occupation</td>
<td>Film director</td>
<td>0.6201</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Jacob Zuma</td>
<td>member of political party</td>
<td>Zulu people</td>
<td>0.5159</td>
<td>False</td>
</tr>
<tr class="even">
<td>Mehmed VI</td>
<td>country of citizenship</td>
<td>Ottoman Empire</td>
<td>0.5479</td>
<td>True</td>
</tr>
<tr class="odd">
<td>Brian Baker</td>
<td>country of citizenship</td>
<td>Australia</td>
<td>0.5523</td>
<td>False</td>
</tr>
<tr class="even">
<td>swimming</td>
<td>occupation</td>
<td>Film director</td>
<td>0.6229</td>
<td>False <!-- ? --></td>
</tr>
<tr class="odd">
<td>West Virginia</td>
<td>member of political party</td>
<td>Tennessee</td>
<td>0.5107</td>
<td>False <!-- ? --></td>
</tr>
<tr class="even">
<td>Tamim bin Hamad Al Thani</td>
<td>country of citizenship</td>
<td>Princely Family of Liechtenstein</td>
<td>0.5289</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Sheldon Whitehouse</td>
<td>member of political party</td>
<td>Washington, D.C.</td>
<td>0.5086</td>
<td>False</td>
</tr>
<tr class="even">
<td>Liberation Tigers of Tamil Eelam</td>
<td>country of citizenship</td>
<td>United States</td>
<td>0.5349</td>
<td>False <!-- ? --></td>
</tr>
<tr class="odd">
<td>Italian Communist Party</td>
<td>country of citizenship</td>
<td>United States</td>
<td>0.5545</td>
<td>False <!-- ? --></td>
</tr>
<tr class="even">
<td>Henri Matisse</td>
<td>country of citizenship</td>
<td>French</td>
<td>0.5004</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Lawrence Ferlinghetti</td>
<td>country of citizenship</td>
<td>United States</td>
<td>0.5471</td>
<td>True</td>
</tr>
<tr class="even">
<td>Michael Andersson</td>
<td>occupation</td>
<td>Film director</td>
<td>0.6283</td>
<td>False</td>
</tr>
<tr class="odd">
<td>Brian Baker</td>
<td>country of citizenship</td>
<td>Canada</td>
<td>0.5187</td>
<td>False</td>
</tr>
<tr class="even">
<td>John E. Sweeney</td>
<td>member of political party</td>
<td>Republican Party</td>
<td>0.5036</td>
<td>True</td>
</tr>
</tbody>
</table>
</table>
<p>Okay sooo… not <em>super</em> impressive, but pretty good for a first shot. At least it does a bit better than rolling a bunch of dice :)</p>
<p>It’s basically a logistic regression over a bag of words. The dataset are 10 000 Wikipedia articles about persons. My task now is to use smarter algorithms to get better results, adding other algorithms (graph patterns and embeddings), running over a larger dataset and fleshing out the architecture.</p>
<p>I’d enjoy talking at length about the various design choices and cool tools I’m using, but I was told 1 A4 page would be quite enough, so I’ll cut my proselytization short just about now.</p>
<p>Let’s get back to work. Have a fine day, and may your values be optimally satisfied!</p>

			</div>
			<hr class="clearfix">
		</div>

		<footer>
			<p>
				The <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a>
				license applies unless otherwise specified.
			</p>

			<p>
				Site proudly generated by
				<a href="http://jaspervdj.be/hakyll">Hakyll</a>.
			</p>
		</footer>

		<!-- Google Analytics tracking -->
		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-17386731-5', 'auto');
			ga('send', 'pageview');
		</script>
	</body>
</html>
